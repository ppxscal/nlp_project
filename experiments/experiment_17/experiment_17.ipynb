{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppxscal/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'ai2_arc_ARC_Challenge_pick_the_most_correct_option')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Initialize the tokenizer and models (one or continuous prompting and other for projected prompting\n",
    "model_projected = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_projected.to(device)\n",
    "\n",
    "# Define the prompt basis\n",
    "prompt_list = [\n",
    "    \"Generate a flowchart to visually represent the logic needed to answer the question\",\n",
    "    \"Write pseudocode for an algorithm that could determine the answer\", \n",
    "    \"Imagine you are explaining the answer to a 5-year-old. Use simple words and analogies.\",\n",
    "    \"Summarize the key insights needed to answer in a short poem\",\n",
    "    \"Draw a concept map connecting all ideas relevant to the question\",\n",
    "    \"List assumptions you must make to provide an answer. What if those assumptions were different?\",\n",
    "    \"Compare and contrast multiple ways to approach the question\", \n",
    "    \"Translate the essence of the question into visual art and describe your interpretation\",\n",
    "    \"Act out an exaggerated skit to depict the logic behind the answer\",\n",
    "    \"Design a decision tree leading to the final answer\",\n",
    "    \"Develop a graphic organizer highlighting relationships between key ideas\",\n",
    "    \"Frame the question from different philosophical perspectives and give each perspective's answer\", \n",
    "    \"Outline an experiment that could empirically validate the answer\",\n",
    "    \"Write a song conveying the concepts needed to respond accurately\",\n",
    "    \"Create a metaphor relating the question to a seemingly unrelated domain\",\n",
    "    \"Prototype a computer program to compute the answer algorithmically\"\n",
    "]\n",
    "\n",
    "print(f'tokenizing prompts')\n",
    "print(f'prompt list length {len(prompt_list)}')\n",
    "\n",
    "basis = tokenizer(prompt_list, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "basis = model_projected.model.shared(basis.input_ids)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['inputs_pretokenized'], truncation=True, padding='max_length')\n",
    "\n",
    "# Apply the function to the dataset\n",
    "print('tokenzing dataset')\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the weight prediction model\n",
    "class LearnWeights(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.5):\n",
    "        super(LearnWeights, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.mean(dim=1, keepdim=True)  # Compute the mean across the token dimension and batch dimension\n",
    "        return x.squeeze(1).mean(dim=0)\n",
    "\n",
    "\n",
    "# Define the projected prompt\n",
    "input_dim = 1024\n",
    "\n",
    "output_dim = len(prompt_list)\n",
    "learn_weights = LearnWeights(input_dim, output_dim).to(device)\n",
    "optimizer_projected = AdamW(learn_weights.parameters())\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "batch_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         new_dataset\u001b[39m.\u001b[39mappend(new_item)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m new_dataset\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m pretokenize_dataset(train_dataset, tokenizer)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m validation_dataset \u001b[39m=\u001b[39m pretokenize_dataset(validation_dataset, tokenizer)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n",
      "\n",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpretokenize_dataset\u001b[39m(dataset, tokenizer, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset[split]\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     new_dataset \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiments/experiment_14/experiment_14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m dataset:\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "# Training loop\n",
    "projected_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "shapes = []\n",
    "\n",
    "def pretokenize_dataset(dataset, tokenizer, split='train'):\n",
    "    dataset = dataset[split]\n",
    "    new_dataset = []\n",
    "    for item in dataset:\n",
    "        new_item = {}\n",
    "        new_item['inputs_tokenized'] = tokenizer.tokenize(item['inputs_pretokenized'])\n",
    "        new_item['targets_tokenized'] = tokenizer.tokenize(item['targets_pretokenized'])\n",
    "        new_dataset.append(new_item)\n",
    "    return new_dataset\n",
    "  \n",
    "  \n",
    "train_dataset = pretokenize_dataset(train_dataset, tokenizer)\n",
    "validation_dataset = pretokenize_dataset(validation_dataset, tokenizer)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_projected = 0\n",
    "    epoch_loss_validation = 0\n",
    "    for _ in range(0, len(train_dataset), batch_size):\n",
    "        i = random.randint(0, len(train_dataset) - batch_size + 1 )\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        if len(batch['inputs_pretokenized']) != batch_size:\n",
    "          continue\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(batch['inputs_tokenized'])\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        labels = tokenizer.convert_tokens_to_ids(batch['targets_tokenized'])\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_projected.model.shared(input_ids)\n",
    "        padding_size = max(0, 150 - input_embeddings.shape[1])\n",
    "        input_embeddings = F.pad(input_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "\n",
    "        # print(f'input embeddings shape' + str(input_embeddings.shape))\n",
    "        weights = learn_weights(input_embeddings)\n",
    "        # print(f'predicted weights' + str(weights))\n",
    "        # print(f'predicted weights shape' + str(weights.shape))\n",
    "        # print(f'basis shape' + str(basis.shape))\n",
    "        \n",
    "        # print(f'soft prompt shape' + str(soft_prompt.shape))\n",
    "        # print(f'soft prompt batch shape' + str(soft_prompt_batch.shape))\n",
    "        projected_prompt_batch = weights.unsqueeze(1).unsqueeze(2).expand_as(basis) * basis\n",
    "        projected_prompt_batch = projected_prompt_batch.sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        # print(f'projected prompt batch shape' + str(projected_prompt_batch.shape))\n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "        \n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "        padding_size = max(0, 250 - combined_projected_embeddings.shape[1])\n",
    "        combined_projected_embeddings = F.pad(combined_projected_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        #print(f'combined projected embeddings shape' + str(combined_projected_embeddings.shape))\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    " \n",
    "\n",
    "        loss_projected = outputs_projected.loss\n",
    "        epoch_loss_projected += loss_projected.item()\n",
    "        \n",
    "\n",
    "        optimizer_projected.zero_grad()\n",
    "        loss_projected.backward(retain_graph=True)\n",
    "        optimizer_projected.step()\n",
    "\n",
    "        #print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        if _ % 200 == 0:\n",
    "          print(f'Epoch {epoch+1}/{epochs}, Batch {_}/{len(train_dataset)}')\n",
    "          print(f'Batch Indices: {[i + k for k in range(i, i+batch_size)]}')\n",
    "          print(f'Loss: {loss_projected.item()}')\n",
    "          print()\n",
    "\n",
    "    print('Validating Epoch...')\n",
    "\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        # Rest of your code...\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(batch['inputs_tokenized'])\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        labels = tokenizer.convert_tokens_to_ids(batch['targets_tokenized'])\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_projected.model.shared(input_ids)\n",
    "        padding_size = max(0, 150 - input_embeddings.shape[1])\n",
    "        input_embeddings = F.pad(input_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "        # print(f'input embeddings shape' + str(input_embeddings.shape))\n",
    "        weights = learn_weights(input_embeddings)\n",
    "        # print(f'predicted weights' + str(weights))\n",
    "        # print(f'predicted weights shape' + str(weights.shape))\n",
    "        # print(f'basis shape' + str(basis.shape))\n",
    "        # print(f'input embeddings shape' + str(input_embeddings.shape))\n",
    "        # print(f'soft prompt shape' + str(soft_prompt.shape))\n",
    "        # print(f'soft prompt batch shape' + str(soft_prompt_batch.shape))\n",
    "        projected_prompt_batch = weights.unsqueeze(1).unsqueeze(2).expand_as(basis) * basis\n",
    "        projected_prompt_batch = projected_prompt_batch.sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        # print(f'projected prompt batch shape' + str(projected_prompt_batch.shape))\n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "\n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "        padding_size = max(0, 250 - combined_projected_embeddings.shape[1])\n",
    "        combined_projected_embeddings = F.pad(combined_projected_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        #print(f'combined projected embeddings shape' + str(combined_projected_embeddings.shape))\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        loss_validation = outputs_projected.loss\n",
    "        epoch_loss_validation += loss_validation.item()\n",
    "\n",
    "    epoch_loss_projected /= (len(list(range(0, len(train_dataset), batch_size))))\n",
    "    epoch_loss_validation /= (len(list(range(0, len(validation_dataset), batch_size))) - 1)\n",
    "\n",
    "    print(f'Epoch Validation Loss: {epoch_loss_validation} \\n', end='')\n",
    "    print()\n",
    "\n",
    "    projected_losses.append(epoch_loss_projected)\n",
    "    validation_losses.append(epoch_loss_validation)\n",
    "\n",
    "    # Create a DataFrame with the loss values\n",
    "    n = len(projected_losses)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n + 1), projected_losses, label='Training')\n",
    "    plt.plot(range(1, n + 1), validation_losses, label='Validation')\n",
    "    \n",
    "    plt.title(f'Normalized Cross Entropy Loss v.s. Epoch for a Learned Linear Combination Model \\n Epochs: {epochs}, Batch Size: {batch_size}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot as a png file\n",
    "    print(f'Saveing figure...')\n",
    "    plt.savefig(f'loss_plot_epoch_{epoch+1}.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
