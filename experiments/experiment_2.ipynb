{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppxscal/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embeddings shape: torch.Size([1, 2, 6, 768]), soft prompt shape: torch.Size([8, 2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the soft prompt\n",
    "L = 8  # Length of the soft prompt\n",
    "d = model.distilbert.embeddings.word_embeddings.embedding_dim  # Dimension of the embeddings\n",
    "soft_prompt = torch.nn.Parameter(torch.randn(L, d))\n",
    "soft_prompt = soft_prompt.unsqueeze(1).repeat(1, 2, 1).unsqueeze(2).repeat(1,1,6,1)\n",
    "\n",
    "\n",
    "# Encode a sample multiple choice question\n",
    "choices = [\"Hello, world!\", \"Goodbye, world!\"]\n",
    "encoded_choices = [tokenizer.encode(s, add_special_tokens=True) for s in choices]\n",
    "input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
    "\n",
    "# Get the input embeddings\n",
    "input_embeddings = model.distilbert.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "print(f'input embeddings shape: {input_embeddings.shape}, soft prompt shape: {soft_prompt.shape}')\n",
    "\n",
    "# Prepend the soft prompt to the input embeddings\n",
    "combined_embeddings = torch.cat([soft_prompt, input_embeddings], dim=0)\n",
    "\n",
    "# Pass the combined embeddings through the model\n",
    "outputs = model(inputs_embeds=combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Here's a question and a few possible answers: \n",
      "\n",
      "Q: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\n",
      "Possible A: bus depot, beach, train station, bridge, bridge\n",
      "\n",
      "Why is \"bridge\" an answer aligned with human common sense? \n",
      "\n",
      "Answer: \n",
      "bums are well known to take up residence under bridges.\n",
      "Dict: {'inputs': [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 29403, 71, 10, 2601, 14089, 6, 2608, 6, 2412, 2478, 6, 4716, 6, 4716, 1615, 19, 96, 9818, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], 'inputs_pretokenized': 'Here\\'s a question and a few possible answers: \\n\\nQ: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nPossible A: bus depot, beach, train station, bridge, bridge\\n\\nWhy is \"bridge\" an answer aligned with human common sense? \\n', 'targets': [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], 'targets_pretokenized': '\\nbums are well known to take up residence under bridges.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDict: \u001b[39m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Tokenize the example\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(example[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Get the input embeddings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m input_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdistilbert\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mword_embeddings(input_ids)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "\n",
    "# Define the soft prompt\n",
    "L = 8  # Length of the soft prompt\n",
    "d = model.distilbert.embeddings.word_embeddings.embedding_dim  # Dimension of the embeddings\n",
    "soft_prompt = torch.nn.Parameter(torch.randn(L, d))\n",
    "\n",
    "# Let's assume we're working with the first example in the dataset\n",
    "example = dataset['train'][1]\n",
    "bro = example[\"inputs_pretokenized\"]\n",
    "moment = example[\"targets_pretokenized\"]\n",
    "\n",
    "print(f'Example: {bro}')\n",
    "print(f'Answer: {moment}')\n",
    "print(f'Dict: {example}')\n",
    "# Tokenize the example\n",
    "input_ids = tokenizer.encode(example['text'], return_tensors='pt')\n",
    "\n",
    "# Get the input embeddings\n",
    "input_embeddings = model.distilbert.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "# Prepend the soft prompt to the input embeddings\n",
    "soft_prompt = soft_prompt.unsqueeze(0).repeat(input_embeddings.size(0), 1, 1)  # Adjust the soft prompt to match the batch size\n",
    "combined_embeddings = torch.cat([soft_prompt, input_embeddings], dim=1)\n",
    "\n",
    "# Pass the combined embeddings through the model\n",
    "outputs = model(inputs_embeds=combined_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
