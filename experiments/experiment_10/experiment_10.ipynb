{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pascal/Projects/nlp_project/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/Pascal/Projects/nlp_project/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing prompts\n",
      "prompt list length 24\n",
      "tokenzing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pascal/Projects/nlp_project/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "epoch 1/5 batch 0/9741loss projected: 7.677109241485596 \n",
      "epoch 1/5 batch 2/9741loss projected: 8.776749610900879 \n",
      "epoch 1/5 batch 4/9741loss projected: 6.486783027648926 \n",
      "epoch 1/5 batch 6/9741loss projected: 6.983078479766846 \n",
      "epoch 1/5 batch 8/9741loss projected: 8.678894996643066 \n",
      "epoch 1/5 batch 10/9741loss projected: 8.298099517822266 \n",
      "epoch 1/5 batch 12/9741loss projected: 7.999495506286621 \n",
      "epoch 1/5 batch 14/9741loss projected: 8.399038314819336 \n",
      "epoch 1/5 batch 16/9741loss projected: 8.313008308410645 \n",
      "epoch 1/5 batch 18/9741loss projected: 6.699790954589844 \n",
      "epoch 1/5 batch 20/9741loss projected: 7.389983654022217 \n",
      "epoch 1/5 batch 22/9741loss projected: 8.068312644958496 \n",
      "epoch 1/5 batch 24/9741loss projected: 10.411550521850586 \n",
      "epoch 1/5 batch 26/9741loss projected: 8.971055030822754 \n",
      "epoch 1/5 batch 28/9741loss projected: 6.808653354644775 \n",
      "epoch 1/5 batch 30/9741loss projected: 8.605818748474121 \n",
      "epoch 1/5 batch 32/9741loss projected: 7.437904357910156 \n",
      "epoch 1/5 batch 34/9741loss projected: 7.979968070983887 \n",
      "epoch 1/5 batch 36/9741loss projected: 8.78031063079834 \n",
      "epoch 1/5 batch 38/9741loss projected: 8.283562660217285 \n",
      "epoch 1/5 batch 40/9741loss projected: 7.694001197814941 \n",
      "epoch 1/5 batch 42/9741loss projected: 8.399020195007324 \n",
      "epoch 1/5 batch 44/9741loss projected: 7.221246719360352 \n",
      "epoch 1/5 batch 46/9741loss projected: 7.795926094055176 \n",
      "epoch 1/5 batch 48/9741loss projected: 7.384192943572998 \n",
      "epoch 1/5 batch 50/9741loss projected: 9.187734603881836 \n",
      "epoch 1/5 batch 52/9741loss projected: 7.868666172027588 \n",
      "epoch 1/5 batch 54/9741loss projected: 7.838837146759033 \n",
      "epoch 1/5 batch 56/9741loss projected: 7.044729232788086 \n",
      "epoch 1/5 batch 58/9741loss projected: 9.238845825195312 \n",
      "epoch 1/5 batch 60/9741loss projected: 8.080835342407227 \n",
      "epoch 1/5 batch 62/9741loss projected: 6.917442321777344 \n",
      "epoch 1/5 batch 64/9741loss projected: 8.794501304626465 \n",
      "epoch 1/5 batch 66/9741loss projected: 9.060864448547363 \n",
      "epoch 1/5 batch 68/9741loss projected: 7.255120754241943 \n",
      "epoch 1/5 batch 70/9741loss projected: 8.565372467041016 \n",
      "epoch 1/5 batch 72/9741loss projected: 7.857271671295166 \n",
      "epoch 1/5 batch 74/9741loss projected: 7.87357234954834 \n",
      "epoch 1/5 batch 76/9741loss projected: 9.518415451049805 \n",
      "epoch 1/5 batch 78/9741loss projected: 10.60595417022705 \n",
      "epoch 1/5 batch 80/9741loss projected: 7.342640399932861 \n",
      "epoch 1/5 batch 82/9741loss projected: 6.987677097320557 \n",
      "epoch 1/5 batch 84/9741loss projected: 8.582185745239258 \n",
      "epoch 1/5 batch 86/9741loss projected: 7.710397720336914 \n",
      "epoch 1/5 batch 88/9741loss projected: 7.207747459411621 \n",
      "epoch 1/5 batch 90/9741loss projected: 8.911569595336914 \n",
      "epoch 1/5 batch 92/9741loss projected: 8.228873252868652 \n",
      "epoch 1/5 batch 94/9741loss projected: 8.958345413208008 \n",
      "epoch 1/5 batch 96/9741loss projected: 6.873868942260742 \n",
      "epoch 1/5 batch 98/9741loss projected: 7.665957927703857 \n",
      "epoch 1/5 batch 100/9741loss projected: 7.029440879821777 \n",
      "epoch 1/5 batch 102/9741loss projected: 6.2490434646606445 \n",
      "epoch 1/5 batch 104/9741loss projected: 11.721465110778809 \n",
      "epoch 1/5 batch 106/9741loss projected: 8.714747428894043 \n",
      "epoch 1/5 batch 108/9741loss projected: 8.873995780944824 \n",
      "epoch 1/5 batch 110/9741loss projected: 6.427648067474365 \n",
      "epoch 1/5 batch 112/9741loss projected: 8.443535804748535 \n",
      "epoch 1/5 batch 114/9741loss projected: 5.569357395172119 \n",
      "epoch 1/5 batch 116/9741loss projected: 8.610261917114258 \n",
      "epoch 1/5 batch 118/9741loss projected: 8.539070129394531 \n",
      "epoch 1/5 batch 120/9741loss projected: 7.469123363494873 \n",
      "epoch 1/5 batch 122/9741loss projected: 7.549178123474121 \n",
      "epoch 1/5 batch 124/9741loss projected: 8.447700500488281 \n",
      "epoch 1/5 batch 126/9741loss projected: 9.326127052307129 \n",
      "epoch 1/5 batch 128/9741loss projected: 8.185162544250488 \n",
      "epoch 1/5 batch 130/9741loss projected: 8.082948684692383 \n",
      "epoch 1/5 batch 132/9741loss projected: 7.447981834411621 \n",
      "epoch 1/5 batch 134/9741loss projected: 11.215876579284668 \n",
      "epoch 1/5 batch 136/9741loss projected: 8.582451820373535 \n",
      "epoch 1/5 batch 138/9741loss projected: 8.6123628616333 \n",
      "epoch 1/5 batch 140/9741loss projected: 9.622967720031738 \n",
      "epoch 1/5 batch 142/9741loss projected: 8.176511764526367 \n",
      "epoch 1/5 batch 144/9741loss projected: 7.986841678619385 \n",
      "epoch 1/5 batch 146/9741loss projected: 7.909694194793701 \n",
      "epoch 1/5 batch 148/9741loss projected: 7.972362995147705 \n",
      "epoch 1/5 batch 150/9741loss projected: 8.072115898132324 \n",
      "epoch 1/5 batch 152/9741loss projected: 8.930388450622559 \n",
      "epoch 1/5 batch 154/9741loss projected: 9.245155334472656 \n",
      "epoch 1/5 batch 156/9741loss projected: 7.549747943878174 \n",
      "epoch 1/5 batch 158/9741loss projected: 8.948526382446289 \n",
      "epoch 1/5 batch 160/9741loss projected: 8.232504844665527 \n",
      "epoch 1/5 batch 162/9741loss projected: 7.278824806213379 \n",
      "epoch 1/5 batch 164/9741loss projected: 8.717219352722168 \n",
      "epoch 1/5 batch 166/9741loss projected: 7.883950710296631 \n",
      "epoch 1/5 batch 168/9741loss projected: 10.636384963989258 \n",
      "epoch 1/5 batch 170/9741loss projected: 9.115206718444824 \n",
      "epoch 1/5 batch 172/9741loss projected: 8.766555786132812 \n",
      "epoch 1/5 batch 174/9741loss projected: 9.554146766662598 \n",
      "epoch 1/5 batch 176/9741loss projected: 7.960670471191406 \n",
      "epoch 1/5 batch 178/9741loss projected: 6.8635029792785645 \n",
      "epoch 1/5 batch 180/9741loss projected: 8.508267402648926 \n",
      "epoch 1/5 batch 182/9741loss projected: 8.16710376739502 \n",
      "epoch 1/5 batch 184/9741loss projected: 7.385579586029053 \n",
      "epoch 1/5 batch 186/9741loss projected: 9.17733383178711 \n",
      "epoch 1/5 batch 188/9741loss projected: 8.990882873535156 \n",
      "epoch 1/5 batch 190/9741loss projected: 5.812229633331299 \n",
      "epoch 1/5 batch 192/9741loss projected: 9.946969985961914 \n",
      "epoch 1/5 batch 194/9741loss projected: 8.878312110900879 \n",
      "epoch 1/5 batch 196/9741loss projected: 7.403151035308838 \n",
      "epoch 1/5 batch 198/9741loss projected: 8.055193901062012 \n",
      "epoch 1/5 batch 200/9741loss projected: 8.396267890930176 \n",
      "epoch 1/5 batch 202/9741loss projected: 8.909207344055176 \n",
      "epoch 1/5 batch 204/9741loss projected: 6.929141998291016 \n",
      "epoch 1/5 batch 206/9741loss projected: 7.476097583770752 \n",
      "epoch 1/5 batch 208/9741loss projected: 7.284397602081299 \n",
      "epoch 1/5 batch 210/9741loss projected: 7.646045207977295 \n",
      "epoch 1/5 batch 212/9741loss projected: 7.448398590087891 \n",
      "epoch 1/5 batch 214/9741loss projected: 8.020522117614746 \n",
      "epoch 1/5 batch 216/9741loss projected: 8.607651710510254 \n",
      "epoch 1/5 batch 218/9741loss projected: 8.192676544189453 \n",
      "epoch 1/5 batch 220/9741loss projected: 7.622901916503906 \n",
      "epoch 1/5 batch 222/9741loss projected: 7.035562515258789 \n",
      "epoch 1/5 batch 224/9741loss projected: 6.2149176597595215 \n",
      "epoch 1/5 batch 226/9741loss projected: 6.938246726989746 \n",
      "epoch 1/5 batch 228/9741loss projected: 8.134516716003418 \n",
      "epoch 1/5 batch 230/9741loss projected: 8.652083396911621 \n",
      "epoch 1/5 batch 232/9741loss projected: 7.984102249145508 \n",
      "epoch 1/5 batch 234/9741loss projected: 6.950747966766357 \n",
      "epoch 1/5 batch 236/9741loss projected: 8.43830394744873 \n",
      "epoch 1/5 batch 238/9741loss projected: 8.498226165771484 \n",
      "epoch 1/5 batch 240/9741loss projected: 9.764846801757812 \n",
      "epoch 1/5 batch 242/9741loss projected: 9.064159393310547 \n",
      "epoch 1/5 batch 244/9741loss projected: 5.943552494049072 \n",
      "epoch 1/5 batch 246/9741loss projected: 7.959240436553955 \n",
      "epoch 1/5 batch 248/9741loss projected: 8.150023460388184 \n",
      "epoch 1/5 batch 250/9741loss projected: 7.713188171386719 \n",
      "epoch 1/5 batch 252/9741loss projected: 7.111039638519287 \n",
      "epoch 1/5 batch 254/9741loss projected: 8.278548240661621 \n",
      "epoch 1/5 batch 256/9741loss projected: 7.560449600219727 \n",
      "epoch 1/5 batch 258/9741loss projected: 6.6215949058532715 \n",
      "epoch 1/5 batch 260/9741loss projected: 7.694019317626953 \n",
      "epoch 1/5 batch 262/9741loss projected: 7.384541988372803 \n",
      "epoch 1/5 batch 264/9741loss projected: 7.527478218078613 \n",
      "epoch 1/5 batch 266/9741loss projected: 9.011290550231934 \n",
      "epoch 1/5 batch 268/9741loss projected: 7.854151725769043 \n",
      "epoch 1/5 batch 270/9741loss projected: 8.203476905822754 \n",
      "epoch 1/5 batch 272/9741loss projected: 8.50780963897705 \n",
      "epoch 1/5 batch 274/9741loss projected: 10.175141334533691 \n",
      "epoch 1/5 batch 276/9741loss projected: 8.591202735900879 \n",
      "epoch 1/5 batch 278/9741loss projected: 6.775209903717041 \n",
      "epoch 1/5 batch 280/9741loss projected: 6.74551248550415 \n",
      "epoch 1/5 batch 282/9741loss projected: 10.32886028289795 \n",
      "epoch 1/5 batch 284/9741loss projected: 5.091933727264404 \n",
      "epoch 1/5 batch 286/9741loss projected: 8.119892120361328 \n",
      "epoch 1/5 batch 288/9741loss projected: 8.959547996520996 \n",
      "epoch 1/5 batch 290/9741loss projected: 7.116546630859375 \n",
      "epoch 1/5 batch 292/9741loss projected: 7.693218231201172 \n",
      "epoch 1/5 batch 294/9741loss projected: 6.287219047546387 \n",
      "epoch 1/5 batch 296/9741loss projected: 9.041492462158203 \n",
      "epoch 1/5 batch 298/9741loss projected: 6.111793518066406 \n",
      "epoch 1/5 batch 300/9741loss projected: 8.46484661102295 \n",
      "epoch 1/5 batch 302/9741loss projected: 7.952996730804443 \n",
      "epoch 1/5 batch 304/9741loss projected: 8.070968627929688 \n",
      "epoch 1/5 batch 306/9741loss projected: 9.22762393951416 \n",
      "epoch 1/5 batch 308/9741loss projected: 7.3206305503845215 \n",
      "epoch 1/5 batch 310/9741loss projected: 8.585537910461426 \n",
      "epoch 1/5 batch 312/9741loss projected: 8.733509063720703 \n",
      "epoch 1/5 batch 314/9741loss projected: 6.809473514556885 \n",
      "epoch 1/5 batch 316/9741loss projected: 9.331840515136719 \n",
      "epoch 1/5 batch 318/9741loss projected: 8.189384460449219 \n",
      "epoch 1/5 batch 320/9741loss projected: 7.503836154937744 \n",
      "epoch 1/5 batch 322/9741loss projected: 7.318021297454834 \n",
      "epoch 1/5 batch 324/9741loss projected: 6.885811805725098 \n",
      "epoch 1/5 batch 326/9741loss projected: 7.625269412994385 \n",
      "epoch 1/5 batch 328/9741loss projected: 8.086219787597656 \n",
      "epoch 1/5 batch 330/9741loss projected: 7.281024932861328 \n",
      "epoch 1/5 batch 332/9741loss projected: 9.158956527709961 \n",
      "epoch 1/5 batch 334/9741loss projected: 7.1630096435546875 \n",
      "epoch 1/5 batch 336/9741loss projected: 8.212890625 \n",
      "epoch 1/5 batch 338/9741loss projected: 8.170759201049805 \n",
      "epoch 1/5 batch 340/9741loss projected: 8.835808753967285 \n",
      "epoch 1/5 batch 342/9741loss projected: 7.358588218688965 \n",
      "epoch 1/5 batch 344/9741loss projected: 8.067665100097656 \n",
      "epoch 1/5 batch 346/9741loss projected: 8.23731803894043 \n",
      "epoch 1/5 batch 348/9741loss projected: 8.561161041259766 \n",
      "epoch 1/5 batch 350/9741loss projected: 8.468071937561035 \n",
      "epoch 1/5 batch 352/9741loss projected: 7.977020740509033 \n",
      "epoch 1/5 batch 354/9741loss projected: 8.233866691589355 \n",
      "epoch 1/5 batch 356/9741loss projected: 8.418548583984375 \n",
      "epoch 1/5 batch 358/9741loss projected: 7.1001505851745605 \n",
      "epoch 1/5 batch 360/9741loss projected: 8.703470230102539 \n",
      "epoch 1/5 batch 362/9741loss projected: 9.144658088684082 \n",
      "epoch 1/5 batch 364/9741loss projected: 7.971532821655273 \n",
      "epoch 1/5 batch 366/9741loss projected: 8.200727462768555 \n",
      "epoch 1/5 batch 368/9741loss projected: 8.768250465393066 \n",
      "epoch 1/5 batch 370/9741loss projected: 7.416311740875244 \n",
      "epoch 1/5 batch 372/9741loss projected: 8.664390563964844 \n",
      "epoch 1/5 batch 374/9741loss projected: 8.773988723754883 \n",
      "epoch 1/5 batch 376/9741loss projected: 6.330244064331055 \n",
      "epoch 1/5 batch 378/9741loss projected: 7.574970722198486 \n",
      "epoch 1/5 batch 380/9741loss projected: 4.655055522918701 \n",
      "epoch 1/5 batch 382/9741loss projected: 9.466413497924805 \n",
      "epoch 1/5 batch 384/9741loss projected: 7.837660312652588 \n",
      "epoch 1/5 batch 386/9741loss projected: 8.885737419128418 \n",
      "epoch 1/5 batch 388/9741loss projected: 8.631563186645508 \n",
      "epoch 1/5 batch 390/9741loss projected: 8.564455032348633 \n",
      "epoch 1/5 batch 392/9741loss projected: 8.669727325439453 \n",
      "epoch 1/5 batch 394/9741loss projected: 9.270630836486816 \n",
      "epoch 1/5 batch 396/9741loss projected: 8.3948392868042 \n",
      "epoch 1/5 batch 398/9741loss projected: 7.061153888702393 \n",
      "epoch 1/5 batch 400/9741loss projected: 9.961455345153809 \n",
      "epoch 1/5 batch 402/9741loss projected: 7.509300231933594 \n",
      "epoch 1/5 batch 404/9741loss projected: 10.315240859985352 \n",
      "epoch 1/5 batch 406/9741loss projected: 7.691461086273193 \n",
      "epoch 1/5 batch 408/9741loss projected: 9.431923866271973 \n",
      "epoch 1/5 batch 410/9741loss projected: 7.742166519165039 \n",
      "epoch 1/5 batch 412/9741loss projected: 8.142114639282227 \n",
      "epoch 1/5 batch 414/9741loss projected: 8.697471618652344 \n",
      "epoch 1/5 batch 416/9741loss projected: 9.103799819946289 \n",
      "epoch 1/5 batch 418/9741loss projected: 7.296552658081055 \n",
      "epoch 1/5 batch 420/9741loss projected: 8.621707916259766 \n",
      "epoch 1/5 batch 422/9741loss projected: 7.878942012786865 \n",
      "epoch 1/5 batch 424/9741loss projected: 7.835887432098389 \n",
      "epoch 1/5 batch 426/9741loss projected: 8.465353012084961 \n",
      "epoch 1/5 batch 428/9741loss projected: 8.907755851745605 \n",
      "epoch 1/5 batch 430/9741loss projected: 8.774678230285645 \n",
      "epoch 1/5 batch 432/9741loss projected: 8.152573585510254 \n",
      "epoch 1/5 batch 434/9741loss projected: 8.288990020751953 \n",
      "epoch 1/5 batch 436/9741loss projected: 7.754889965057373 \n",
      "epoch 1/5 batch 438/9741loss projected: 9.39435863494873 \n",
      "epoch 1/5 batch 440/9741loss projected: 7.640815258026123 \n",
      "epoch 1/5 batch 442/9741loss projected: 8.49294376373291 \n",
      "epoch 1/5 batch 444/9741loss projected: 8.20279598236084 \n",
      "epoch 1/5 batch 446/9741loss projected: 9.161069869995117 \n",
      "epoch 1/5 batch 448/9741loss projected: 7.6663618087768555 \n",
      "epoch 1/5 batch 450/9741loss projected: 8.811071395874023 \n",
      "epoch 1/5 batch 452/9741loss projected: 7.875943660736084 \n",
      "epoch 1/5 batch 454/9741loss projected: 9.745268821716309 \n",
      "epoch 1/5 batch 456/9741loss projected: 6.008402347564697 \n",
      "epoch 1/5 batch 458/9741loss projected: 9.248907089233398 \n",
      "epoch 1/5 batch 460/9741loss projected: 8.16043472290039 \n",
      "epoch 1/5 batch 462/9741loss projected: 10.028641700744629 \n",
      "epoch 1/5 batch 464/9741loss projected: 9.98670768737793 \n",
      "epoch 1/5 batch 466/9741loss projected: 8.828948020935059 \n",
      "epoch 1/5 batch 468/9741loss projected: 8.109831809997559 \n",
      "epoch 1/5 batch 470/9741loss projected: 8.405862808227539 \n",
      "epoch 1/5 batch 472/9741loss projected: 9.371200561523438 \n",
      "epoch 1/5 batch 474/9741loss projected: 7.236333847045898 \n",
      "epoch 1/5 batch 476/9741loss projected: 8.891721725463867 \n",
      "epoch 1/5 batch 478/9741loss projected: 5.68986701965332 \n",
      "epoch 1/5 batch 480/9741loss projected: 7.160982608795166 \n",
      "epoch 1/5 batch 482/9741loss projected: 7.498409748077393 \n",
      "epoch 1/5 batch 484/9741loss projected: 8.395306587219238 \n",
      "epoch 1/5 batch 486/9741loss projected: 8.43581771850586 \n",
      "epoch 1/5 batch 488/9741loss projected: 7.3580780029296875 \n",
      "epoch 1/5 batch 490/9741loss projected: 6.435142517089844 \n",
      "epoch 1/5 batch 492/9741loss projected: 8.373743057250977 \n",
      "epoch 1/5 batch 494/9741loss projected: 7.14921760559082 \n",
      "epoch 1/5 batch 496/9741loss projected: 8.592307090759277 \n",
      "epoch 1/5 batch 498/9741loss projected: 8.043716430664062 \n",
      "epoch 1/5 batch 500/9741loss projected: 7.704832077026367 \n",
      "epoch 1/5 batch 502/9741loss projected: 7.255281448364258 \n",
      "epoch 1/5 batch 504/9741loss projected: 9.552688598632812 \n",
      "epoch 1/5 batch 506/9741loss projected: 8.76372241973877 \n",
      "epoch 1/5 batch 508/9741loss projected: 7.9785356521606445 \n",
      "epoch 1/5 batch 510/9741loss projected: 8.285170555114746 \n",
      "epoch 1/5 batch 512/9741loss projected: 9.502421379089355 \n",
      "epoch 1/5 batch 514/9741loss projected: 8.529643058776855 \n",
      "epoch 1/5 batch 516/9741loss projected: 5.908866882324219 \n",
      "epoch 1/5 batch 518/9741loss projected: 8.488363265991211 \n",
      "epoch 1/5 batch 520/9741loss projected: 8.425612449645996 \n",
      "epoch 1/5 batch 522/9741loss projected: 8.627622604370117 \n",
      "epoch 1/5 batch 524/9741loss projected: 8.223783493041992 \n",
      "epoch 1/5 batch 526/9741loss projected: 5.086891174316406 \n",
      "epoch 1/5 batch 528/9741loss projected: 6.517912864685059 \n",
      "epoch 1/5 batch 530/9741loss projected: 6.190834999084473 \n",
      "epoch 1/5 batch 532/9741loss projected: 7.612132549285889 \n",
      "epoch 1/5 batch 534/9741loss projected: 7.321499824523926 \n",
      "epoch 1/5 batch 536/9741loss projected: 8.794534683227539 \n",
      "epoch 1/5 batch 538/9741loss projected: 8.300877571105957 \n",
      "epoch 1/5 batch 540/9741loss projected: 8.690099716186523 \n",
      "epoch 1/5 batch 542/9741loss projected: 8.161054611206055 \n",
      "epoch 1/5 batch 544/9741loss projected: 6.7185139656066895 \n",
      "epoch 1/5 batch 546/9741loss projected: 9.131563186645508 \n",
      "epoch 1/5 batch 548/9741loss projected: 7.953612804412842 \n",
      "epoch 1/5 batch 550/9741loss projected: 10.311548233032227 \n",
      "epoch 1/5 batch 552/9741loss projected: 8.315191268920898 \n",
      "epoch 1/5 batch 554/9741loss projected: 8.171647071838379 \n",
      "epoch 1/5 batch 556/9741loss projected: 8.128252029418945 \n",
      "epoch 1/5 batch 558/9741loss projected: 7.2432332038879395 \n",
      "epoch 1/5 batch 560/9741loss projected: 7.707620143890381 \n",
      "epoch 1/5 batch 562/9741loss projected: 4.883594989776611 \n",
      "epoch 1/5 batch 564/9741loss projected: 6.1295905113220215 \n",
      "epoch 1/5 batch 566/9741loss projected: 7.199828147888184 \n",
      "epoch 1/5 batch 568/9741loss projected: 8.576850891113281 \n",
      "epoch 1/5 batch 570/9741loss projected: 4.07543420791626 \n",
      "epoch 1/5 batch 572/9741loss projected: 7.998950004577637 \n",
      "epoch 1/5 batch 574/9741loss projected: 7.682006359100342 \n",
      "epoch 1/5 batch 576/9741loss projected: 8.008909225463867 \n",
      "epoch 1/5 batch 578/9741loss projected: 9.032602310180664 \n",
      "epoch 1/5 batch 580/9741loss projected: 8.224654197692871 \n",
      "epoch 1/5 batch 582/9741loss projected: 8.939810752868652 \n",
      "epoch 1/5 batch 584/9741loss projected: 8.394298553466797 \n",
      "epoch 1/5 batch 586/9741loss projected: 7.8858842849731445 \n",
      "epoch 1/5 batch 588/9741loss projected: 8.935418128967285 \n",
      "epoch 1/5 batch 590/9741loss projected: 6.104609489440918 \n",
      "epoch 1/5 batch 592/9741loss projected: 9.362323760986328 \n",
      "epoch 1/5 batch 594/9741loss projected: 7.227110862731934 \n",
      "epoch 1/5 batch 596/9741loss projected: 8.822477340698242 \n",
      "epoch 1/5 batch 598/9741loss projected: 7.474836826324463 \n",
      "epoch 1/5 batch 600/9741loss projected: 8.991252899169922 \n",
      "epoch 1/5 batch 602/9741loss projected: 6.480839252471924 \n",
      "epoch 1/5 batch 604/9741loss projected: 7.997350692749023 \n",
      "epoch 1/5 batch 606/9741loss projected: 7.640758991241455 \n",
      "epoch 1/5 batch 608/9741loss projected: 7.965593338012695 \n",
      "epoch 1/5 batch 610/9741loss projected: 9.197766304016113 \n",
      "epoch 1/5 batch 612/9741loss projected: 8.554182052612305 \n",
      "epoch 1/5 batch 614/9741loss projected: 6.991420745849609 \n",
      "epoch 1/5 batch 616/9741loss projected: 8.847617149353027 \n",
      "epoch 1/5 batch 618/9741loss projected: 9.017557144165039 \n",
      "epoch 1/5 batch 620/9741loss projected: 9.123600959777832 \n",
      "epoch 1/5 batch 622/9741loss projected: 8.62962532043457 \n",
      "epoch 1/5 batch 624/9741loss projected: 7.81356954574585 \n",
      "epoch 1/5 batch 626/9741loss projected: 7.116122722625732 \n",
      "epoch 1/5 batch 628/9741loss projected: 7.093663692474365 \n",
      "epoch 1/5 batch 630/9741loss projected: 6.747189044952393 \n",
      "epoch 1/5 batch 632/9741loss projected: 6.533810615539551 \n",
      "epoch 1/5 batch 634/9741loss projected: 7.824212551116943 \n",
      "epoch 1/5 batch 636/9741loss projected: 7.963075637817383 \n",
      "epoch 1/5 batch 638/9741loss projected: 7.1386590003967285 \n",
      "epoch 1/5 batch 640/9741loss projected: 7.806159973144531 \n",
      "epoch 1/5 batch 642/9741loss projected: 7.077977657318115 \n",
      "epoch 1/5 batch 644/9741loss projected: 6.9373273849487305 \n",
      "epoch 1/5 batch 646/9741loss projected: 7.808961868286133 \n",
      "epoch 1/5 batch 648/9741loss projected: 7.151273250579834 \n",
      "epoch 1/5 batch 650/9741loss projected: 8.572844505310059 \n",
      "epoch 1/5 batch 652/9741loss projected: 8.844120979309082 \n",
      "epoch 1/5 batch 654/9741loss projected: 8.807839393615723 \n",
      "epoch 1/5 batch 656/9741loss projected: 7.973804950714111 \n",
      "epoch 1/5 batch 658/9741loss projected: 8.910289764404297 \n",
      "epoch 1/5 batch 660/9741loss projected: 7.231039524078369 \n",
      "epoch 1/5 batch 662/9741loss projected: 10.437077522277832 \n",
      "epoch 1/5 batch 664/9741loss projected: 5.834392547607422 \n",
      "epoch 1/5 batch 666/9741loss projected: 6.902517795562744 \n",
      "epoch 1/5 batch 668/9741loss projected: 7.079041957855225 \n",
      "epoch 1/5 batch 670/9741loss projected: 3.6772258281707764 \n",
      "epoch 1/5 batch 672/9741loss projected: 7.266861438751221 \n",
      "epoch 1/5 batch 674/9741loss projected: 7.497243404388428 \n",
      "epoch 1/5 batch 676/9741loss projected: 7.800907611846924 \n",
      "epoch 1/5 batch 678/9741loss projected: 7.837589740753174 \n",
      "epoch 1/5 batch 680/9741loss projected: 9.890244483947754 \n",
      "epoch 1/5 batch 682/9741loss projected: 7.041076183319092 \n",
      "epoch 1/5 batch 684/9741loss projected: 7.796196937561035 \n",
      "epoch 1/5 batch 686/9741loss projected: 7.774561882019043 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m epoch_loss_projected \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_projected\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m optimizer_projected\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m loss_projected\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m optimizer_projected\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m#print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Initialize the tokenizer and models (one or continuous prompting and other for projected prompting\n",
    "model_projected = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_projected.to(device)\n",
    "\n",
    "# Define the prompt basis\n",
    "prompt_list = [\n",
    "    'When you see the following question, I would like you to answer it correctly',\n",
    "    'Produce an executable artifact of type X that will answer the question, and then execute it',\n",
    "    'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When you then answered the three questions, combine the answers to produce the final answers to my original question',\n",
    "    'Generate a set of facts that are contained in the output. The set of facts should be inserted in a specific point in the output to answer the question',\n",
    "    'Given the following question, generate a detailed explanation before providing the correct answer',\n",
    "    'Imagine you are a teacher explaining the answer to this question to a student. How would you respond?',\n",
    "    'Consider the following question. What are the key concepts involved and how do they lead to the correct answer?',\n",
    "    'As an expert in the field, how would you respond to the following question?',\n",
    "    'Translate the following question into a simpler form, then provide the answer',\n",
    "    'If you were to create a diagram to answer this question, what would it look like? Describe it in detail',\n",
    "    'Pretend you are explaining the answer to this question to someone with no background in the subject. How would you explain it?',\n",
    "    'As a highly proficient translator, translate the following question into a different context, then provide the answer',\n",
    "    'Generate a step-by-step guide to answer the following question',\n",
    "    'Consider the following question. What assumptions are you making in order to answer it?',\n",
    "    'If you were to debate the answer to this question, what points would you raise?',\n",
    "    'As a researcher, how would you investigate the answer to the following question?',\n",
    "    'Pretend you are a journalist reporting on the answer to this question. How would you present it?',\n",
    "    'As a storyteller, weave a narrative around the answer to this question',\n",
    "    'If you were to answer this question in a court of law, what evidence would you present?',\n",
    "    'As a detective, how would you piece together the answer to this question?',\n",
    "    'Imagine you are a computer program designed to answer this question. What algorithms or processes would you use?',\n",
    "    'As a philosopher, how would you interpret the answer to this question?',\n",
    "    'If you were to answer this question in a job interview, how would you respond?',\n",
    "    'As a scientist, how would you experiment to find the answer to this question?'\n",
    "]\n",
    "\n",
    "print(f'tokenizing prompts')\n",
    "print(f'prompt list length {len(prompt_list)}')\n",
    "\n",
    "basis = tokenizer(prompt_list, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "basis = model_projected.model.shared(basis.input_ids)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['inputs_pretokenized'], truncation=True, padding='max_length')\n",
    "\n",
    "# Apply the function to the dataset\n",
    "print('tokenzing dataset')\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the weight prediction model\n",
    "class LearnWeights(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LearnWeights, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = x.mean(dim=1, keepdim=True)  # Compute the mean across the token dimension and batch dimension\n",
    "        return x.squeeze(1).mean(dim=0)\n",
    "        \n",
    "       \n",
    "\n",
    "# Define the projected prompt\n",
    "input_dim = 1024\n",
    "\n",
    "output_dim = len(prompt_list)\n",
    "learn_weights = LearnWeights(input_dim, output_dim).to(device)\n",
    "optimizer_projected = AdamW(learn_weights.parameters())\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "batch_size = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('starting training')\n",
    "\n",
    "# Training loop\n",
    "projected_losses = []\n",
    "\n",
    "shapes = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_continuous = 0\n",
    "    epoch_loss_projected = 0\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_projected.model.shared(input_ids)\n",
    "        padding_size = max(0, 100 - input_embeddings.shape[1])\n",
    "        input_embeddings = F.pad(input_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "\n",
    "        \n",
    "        weights = learn_weights(input_embeddings)\n",
    "        # print(f'predicted weights' + str(weights))\n",
    "        # print(f'predicted weights shape' + str(weights.shape))\n",
    "        # print(f'basis shape' + str(basis.shape))\n",
    "        # print(f'input embeddings shape' + str(input_embeddings.shape))\n",
    "        # print(f'soft prompt shape' + str(soft_prompt.shape))\n",
    "        # print(f'soft prompt batch shape' + str(soft_prompt_batch.shape))\n",
    "        projected_prompt_batch = weights.unsqueeze(1).unsqueeze(2).expand_as(basis) * basis\n",
    "        projected_prompt_batch = projected_prompt_batch.sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        # print(f'projected prompt batch shape' + str(projected_prompt_batch.shape))\n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "        \n",
    "    \n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "\n",
    "        loss_projected = outputs_projected.loss\n",
    "        epoch_loss_projected += loss_projected.item()\n",
    "\n",
    "        optimizer_projected.zero_grad()\n",
    "        loss_projected.backward(retain_graph=True)\n",
    "        optimizer_projected.step()\n",
    "\n",
    "        #print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'epoch {epoch+1}/{epochs} batch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'loss projected: {loss_projected.item()} \\n', end='')\n",
    "        \n",
    "\n",
    "    epoch_loss_continuous /= len(train_dataset)\n",
    "    epoch_loss_projected /= len(train_dataset)\n",
    "\n",
    "    projected_losses.append(epoch_loss_projected)\n",
    "\n",
    "    # Create a DataFrame with the loss values\n",
    "    n = len(projected_losses)\n",
    "    data = {\n",
    "        'Epoch': list(range(1, n + 1)),\n",
    "        'Loss': projected_losses,\n",
    "        'Model': ['Projected'] * n\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Epoch', y='Loss', hue='Model')\n",
    "    plt.title('Loss per Epoch for Continuous and Projected Models')\n",
    "\n",
    "    # Save the plot as an SVG file\n",
    "    plt.savefig(f'/Users/Pascal/Projects/nlp_project/experiments/experiment_10/loss_plot_epoch_{epoch+1}.svg', format='svg')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
