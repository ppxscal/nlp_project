{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing prompts\n",
      "prompt list length 24\n",
      "tokenzing dataset\n",
      "starting training\n",
      "epoch 1/10 batch 0/9741loss projected: 7.988592624664307 \n",
      "epoch 1/10 batch 2/9741loss projected: 8.971263885498047 \n",
      "epoch 1/10 batch 4/9741loss projected: 6.821470260620117 \n",
      "epoch 1/10 batch 6/9741loss projected: 7.251396656036377 \n",
      "epoch 1/10 batch 8/9741loss projected: 8.676923751831055 \n",
      "epoch 1/10 batch 10/9741loss projected: 8.299203872680664 \n",
      "epoch 1/10 batch 12/9741loss projected: 8.412371635437012 \n",
      "epoch 1/10 batch 14/9741loss projected: 8.902032852172852 \n",
      "epoch 1/10 batch 16/9741loss projected: 8.449466705322266 \n",
      "epoch 1/10 batch 18/9741loss projected: 7.155532360076904 \n",
      "epoch 1/10 batch 20/9741loss projected: 7.6706390380859375 \n",
      "epoch 1/10 batch 22/9741loss projected: 8.022941589355469 \n",
      "epoch 1/10 batch 24/9741loss projected: 10.7752046585083 \n",
      "epoch 1/10 batch 26/9741loss projected: 9.293580055236816 \n",
      "epoch 1/10 batch 28/9741loss projected: 6.995113849639893 \n",
      "epoch 1/10 batch 30/9741loss projected: 9.693085670471191 \n",
      "epoch 1/10 batch 32/9741loss projected: 8.127745628356934 \n",
      "epoch 1/10 batch 34/9741loss projected: 8.945650100708008 \n",
      "epoch 1/10 batch 36/9741loss projected: 8.959393501281738 \n",
      "epoch 1/10 batch 38/9741loss projected: 8.273421287536621 \n",
      "epoch 1/10 batch 40/9741loss projected: 7.835959434509277 \n",
      "epoch 1/10 batch 42/9741loss projected: 8.805209159851074 \n",
      "epoch 1/10 batch 44/9741loss projected: 7.311261177062988 \n",
      "epoch 1/10 batch 46/9741loss projected: 8.22994327545166 \n",
      "epoch 1/10 batch 48/9741loss projected: 7.43788480758667 \n",
      "epoch 1/10 batch 50/9741loss projected: 9.089897155761719 \n",
      "epoch 1/10 batch 52/9741loss projected: 8.335229873657227 \n",
      "epoch 1/10 batch 54/9741loss projected: 8.450179100036621 \n",
      "epoch 1/10 batch 56/9741loss projected: 7.141973495483398 \n",
      "epoch 1/10 batch 58/9741loss projected: 9.428010940551758 \n",
      "epoch 1/10 batch 60/9741loss projected: 8.066841125488281 \n",
      "epoch 1/10 batch 62/9741loss projected: 6.972376346588135 \n",
      "epoch 1/10 batch 64/9741loss projected: 8.918251991271973 \n",
      "epoch 1/10 batch 66/9741loss projected: 9.277899742126465 \n",
      "epoch 1/10 batch 68/9741loss projected: 7.438889980316162 \n",
      "epoch 1/10 batch 70/9741loss projected: 8.443381309509277 \n",
      "epoch 1/10 batch 72/9741loss projected: 8.064854621887207 \n",
      "epoch 1/10 batch 74/9741loss projected: 7.957317352294922 \n",
      "epoch 1/10 batch 76/9741loss projected: 10.000387191772461 \n",
      "epoch 1/10 batch 78/9741loss projected: 10.806647300720215 \n",
      "epoch 1/10 batch 80/9741loss projected: 7.689567565917969 \n",
      "epoch 1/10 batch 82/9741loss projected: 7.381541728973389 \n",
      "epoch 1/10 batch 84/9741loss projected: 8.529210090637207 \n",
      "epoch 1/10 batch 86/9741loss projected: 7.821812629699707 \n",
      "epoch 1/10 batch 88/9741loss projected: 7.624560356140137 \n",
      "epoch 1/10 batch 90/9741loss projected: 8.80976676940918 \n",
      "epoch 1/10 batch 92/9741loss projected: 8.080412864685059 \n",
      "epoch 1/10 batch 94/9741loss projected: 8.757205963134766 \n",
      "epoch 1/10 batch 96/9741loss projected: 7.161361217498779 \n",
      "epoch 1/10 batch 98/9741loss projected: 8.268213272094727 \n",
      "epoch 1/10 batch 100/9741loss projected: 6.9710588455200195 \n",
      "epoch 1/10 batch 102/9741loss projected: 6.440314769744873 \n",
      "epoch 1/10 batch 104/9741loss projected: 11.75915813446045 \n",
      "epoch 1/10 batch 106/9741loss projected: 9.075850486755371 \n",
      "epoch 1/10 batch 108/9741loss projected: 8.806994438171387 \n",
      "epoch 1/10 batch 110/9741loss projected: 6.8524298667907715 \n",
      "epoch 1/10 batch 112/9741loss projected: 8.284247398376465 \n",
      "epoch 1/10 batch 114/9741loss projected: 5.844949245452881 \n",
      "epoch 1/10 batch 116/9741loss projected: 8.641807556152344 \n",
      "epoch 1/10 batch 118/9741loss projected: 8.569112777709961 \n",
      "epoch 1/10 batch 120/9741loss projected: 7.409041881561279 \n",
      "epoch 1/10 batch 122/9741loss projected: 7.514872074127197 \n",
      "epoch 1/10 batch 124/9741loss projected: 8.638786315917969 \n",
      "epoch 1/10 batch 126/9741loss projected: 9.134108543395996 \n",
      "epoch 1/10 batch 128/9741loss projected: 8.283221244812012 \n",
      "epoch 1/10 batch 130/9741loss projected: 8.00955867767334 \n",
      "epoch 1/10 batch 132/9741loss projected: 7.456731796264648 \n",
      "epoch 1/10 batch 134/9741loss projected: 10.986165046691895 \n",
      "epoch 1/10 batch 136/9741loss projected: 8.824858665466309 \n",
      "epoch 1/10 batch 138/9741loss projected: 8.370850563049316 \n",
      "epoch 1/10 batch 140/9741loss projected: 9.694045066833496 \n",
      "epoch 1/10 batch 142/9741loss projected: 8.184240341186523 \n",
      "epoch 1/10 batch 144/9741loss projected: 8.111254692077637 \n",
      "epoch 1/10 batch 146/9741loss projected: 7.705739974975586 \n",
      "epoch 1/10 batch 148/9741loss projected: 8.393423080444336 \n",
      "epoch 1/10 batch 150/9741loss projected: 8.416728973388672 \n",
      "epoch 1/10 batch 152/9741loss projected: 9.130895614624023 \n",
      "epoch 1/10 batch 154/9741loss projected: 9.154363632202148 \n",
      "epoch 1/10 batch 156/9741loss projected: 8.410628318786621 \n",
      "epoch 1/10 batch 158/9741loss projected: 9.479860305786133 \n",
      "epoch 1/10 batch 160/9741loss projected: 8.254785537719727 \n",
      "epoch 1/10 batch 162/9741loss projected: 8.770187377929688 \n",
      "epoch 1/10 batch 164/9741loss projected: 8.938672065734863 \n",
      "epoch 1/10 batch 166/9741loss projected: 8.385683059692383 \n",
      "epoch 1/10 batch 168/9741loss projected: 10.52018928527832 \n",
      "epoch 1/10 batch 170/9741loss projected: 9.176292419433594 \n",
      "epoch 1/10 batch 172/9741loss projected: 9.214834213256836 \n",
      "epoch 1/10 batch 174/9741loss projected: 10.026248931884766 \n",
      "epoch 1/10 batch 176/9741loss projected: 8.515393257141113 \n",
      "epoch 1/10 batch 178/9741loss projected: 7.4831318855285645 \n",
      "epoch 1/10 batch 180/9741loss projected: 8.544885635375977 \n",
      "epoch 1/10 batch 182/9741loss projected: 8.144904136657715 \n",
      "epoch 1/10 batch 184/9741loss projected: 8.057120323181152 \n",
      "epoch 1/10 batch 186/9741loss projected: 9.89605712890625 \n",
      "epoch 1/10 batch 188/9741loss projected: 8.990954399108887 \n",
      "epoch 1/10 batch 190/9741loss projected: 5.994165420532227 \n",
      "epoch 1/10 batch 192/9741loss projected: 9.766725540161133 \n",
      "epoch 1/10 batch 194/9741loss projected: 8.881640434265137 \n",
      "epoch 1/10 batch 196/9741loss projected: 7.793439865112305 \n",
      "epoch 1/10 batch 198/9741loss projected: 8.208442687988281 \n",
      "epoch 1/10 batch 200/9741loss projected: 9.008492469787598 \n",
      "epoch 1/10 batch 202/9741loss projected: 8.950268745422363 \n",
      "epoch 1/10 batch 204/9741loss projected: 7.263092994689941 \n",
      "epoch 1/10 batch 206/9741loss projected: 7.722423553466797 \n",
      "epoch 1/10 batch 208/9741loss projected: 7.523110866546631 \n",
      "epoch 1/10 batch 210/9741loss projected: 8.123693466186523 \n",
      "epoch 1/10 batch 212/9741loss projected: 7.645777702331543 \n",
      "epoch 1/10 batch 214/9741loss projected: 8.86245346069336 \n",
      "epoch 1/10 batch 216/9741loss projected: 8.63061237335205 \n",
      "epoch 1/10 batch 218/9741loss projected: 8.253554344177246 \n",
      "epoch 1/10 batch 220/9741loss projected: 7.971465110778809 \n",
      "epoch 1/10 batch 222/9741loss projected: 7.643742084503174 \n",
      "epoch 1/10 batch 224/9741loss projected: 6.710956573486328 \n",
      "epoch 1/10 batch 226/9741loss projected: 7.050168037414551 \n",
      "epoch 1/10 batch 228/9741loss projected: 8.540063858032227 \n",
      "epoch 1/10 batch 230/9741loss projected: 8.639629364013672 \n",
      "epoch 1/10 batch 232/9741loss projected: 8.23306655883789 \n",
      "epoch 1/10 batch 234/9741loss projected: 7.312282085418701 \n",
      "epoch 1/10 batch 236/9741loss projected: 8.784126281738281 \n",
      "epoch 1/10 batch 238/9741loss projected: 8.911931037902832 \n",
      "epoch 1/10 batch 240/9741loss projected: 9.969561576843262 \n",
      "epoch 1/10 batch 242/9741loss projected: 9.295357704162598 \n",
      "epoch 1/10 batch 244/9741loss projected: 6.245728969573975 \n",
      "epoch 1/10 batch 246/9741loss projected: 8.83944034576416 \n",
      "epoch 1/10 batch 248/9741loss projected: 8.03134536743164 \n",
      "epoch 1/10 batch 250/9741loss projected: 7.723852634429932 \n",
      "epoch 1/10 batch 252/9741loss projected: 7.449256896972656 \n",
      "epoch 1/10 batch 254/9741loss projected: 8.326485633850098 \n",
      "epoch 1/10 batch 256/9741loss projected: 8.259267807006836 \n",
      "epoch 1/10 batch 258/9741loss projected: 7.361407279968262 \n",
      "epoch 1/10 batch 260/9741loss projected: 9.333246231079102 \n",
      "epoch 1/10 batch 262/9741loss projected: 7.597031116485596 \n",
      "epoch 1/10 batch 264/9741loss projected: 7.892124652862549 \n",
      "epoch 1/10 batch 266/9741loss projected: 9.2974271774292 \n",
      "epoch 1/10 batch 268/9741loss projected: 8.729384422302246 \n",
      "epoch 1/10 batch 270/9741loss projected: 9.54632568359375 \n",
      "epoch 1/10 batch 272/9741loss projected: 8.726861953735352 \n",
      "epoch 1/10 batch 274/9741loss projected: 11.187765121459961 \n",
      "epoch 1/10 batch 276/9741loss projected: 9.317728996276855 \n",
      "epoch 1/10 batch 278/9741loss projected: 8.37564468383789 \n",
      "epoch 1/10 batch 280/9741loss projected: 6.911739349365234 \n",
      "epoch 1/10 batch 282/9741loss projected: 11.248112678527832 \n",
      "epoch 1/10 batch 284/9741loss projected: 6.931728839874268 \n",
      "epoch 1/10 batch 286/9741loss projected: 8.32961368560791 \n",
      "epoch 1/10 batch 288/9741loss projected: 9.056259155273438 \n",
      "epoch 1/10 batch 290/9741loss projected: 7.89870548248291 \n",
      "epoch 1/10 batch 292/9741loss projected: 8.17306900024414 \n",
      "epoch 1/10 batch 294/9741loss projected: 6.486717224121094 \n",
      "epoch 1/10 batch 296/9741loss projected: 9.05906867980957 \n",
      "epoch 1/10 batch 298/9741loss projected: 6.348977565765381 \n",
      "epoch 1/10 batch 300/9741loss projected: 8.887520790100098 \n",
      "epoch 1/10 batch 302/9741loss projected: 8.62468433380127 \n",
      "epoch 1/10 batch 304/9741loss projected: 8.138915061950684 \n",
      "epoch 1/10 batch 306/9741loss projected: 9.32624340057373 \n",
      "epoch 1/10 batch 308/9741loss projected: 7.412893772125244 \n",
      "epoch 1/10 batch 310/9741loss projected: 8.482880592346191 \n",
      "epoch 1/10 batch 312/9741loss projected: 8.53511905670166 \n",
      "epoch 1/10 batch 314/9741loss projected: 6.836684226989746 \n",
      "epoch 1/10 batch 316/9741loss projected: 9.587777137756348 \n",
      "epoch 1/10 batch 318/9741loss projected: 8.566097259521484 \n",
      "epoch 1/10 batch 320/9741loss projected: 7.6690754890441895 \n",
      "epoch 1/10 batch 322/9741loss projected: 7.84247350692749 \n",
      "epoch 1/10 batch 324/9741loss projected: 6.949056625366211 \n",
      "epoch 1/10 batch 326/9741loss projected: 8.516536712646484 \n",
      "epoch 1/10 batch 328/9741loss projected: 8.368524551391602 \n",
      "epoch 1/10 batch 330/9741loss projected: 8.218074798583984 \n",
      "epoch 1/10 batch 332/9741loss projected: 9.53685474395752 \n",
      "epoch 1/10 batch 334/9741loss projected: 7.315375328063965 \n",
      "epoch 1/10 batch 336/9741loss projected: 8.86194896697998 \n",
      "epoch 1/10 batch 338/9741loss projected: 8.861281394958496 \n",
      "epoch 1/10 batch 340/9741loss projected: 8.752859115600586 \n",
      "epoch 1/10 batch 342/9741loss projected: 8.715583801269531 \n",
      "epoch 1/10 batch 344/9741loss projected: 9.221085548400879 \n",
      "epoch 1/10 batch 346/9741loss projected: 8.917938232421875 \n",
      "epoch 1/10 batch 348/9741loss projected: 8.995875358581543 \n",
      "epoch 1/10 batch 350/9741loss projected: 9.38557243347168 \n",
      "epoch 1/10 batch 352/9741loss projected: 8.049304962158203 \n",
      "epoch 1/10 batch 354/9741loss projected: 8.491827011108398 \n",
      "epoch 1/10 batch 356/9741loss projected: 8.375761985778809 \n",
      "epoch 1/10 batch 358/9741loss projected: 7.082202911376953 \n",
      "epoch 1/10 batch 360/9741loss projected: 9.516549110412598 \n",
      "epoch 1/10 batch 362/9741loss projected: 9.742050170898438 \n",
      "epoch 1/10 batch 364/9741loss projected: 8.410868644714355 \n",
      "epoch 1/10 batch 366/9741loss projected: 8.739093780517578 \n",
      "epoch 1/10 batch 368/9741loss projected: 8.86418628692627 \n",
      "epoch 1/10 batch 370/9741loss projected: 7.392346382141113 \n",
      "epoch 1/10 batch 372/9741loss projected: 9.257187843322754 \n",
      "epoch 1/10 batch 374/9741loss projected: 9.10735034942627 \n",
      "epoch 1/10 batch 376/9741loss projected: 7.038632869720459 \n",
      "epoch 1/10 batch 378/9741loss projected: 7.594370365142822 \n",
      "epoch 1/10 batch 380/9741loss projected: 6.411092281341553 \n",
      "epoch 1/10 batch 382/9741loss projected: 9.620222091674805 \n",
      "epoch 1/10 batch 384/9741loss projected: 8.287529945373535 \n",
      "epoch 1/10 batch 386/9741loss projected: 9.11302375793457 \n",
      "epoch 1/10 batch 388/9741loss projected: 8.478111267089844 \n",
      "epoch 1/10 batch 390/9741loss projected: 9.184761047363281 \n",
      "epoch 1/10 batch 392/9741loss projected: 9.524446487426758 \n",
      "epoch 1/10 batch 394/9741loss projected: 9.075993537902832 \n",
      "epoch 1/10 batch 396/9741loss projected: 8.619893074035645 \n",
      "epoch 1/10 batch 398/9741loss projected: 7.231179714202881 \n",
      "epoch 1/10 batch 400/9741loss projected: 9.985296249389648 \n",
      "epoch 1/10 batch 402/9741loss projected: 8.039597511291504 \n",
      "epoch 1/10 batch 404/9741loss projected: 10.208789825439453 \n",
      "epoch 1/10 batch 406/9741loss projected: 8.33071231842041 \n",
      "epoch 1/10 batch 408/9741loss projected: 9.522153854370117 \n",
      "epoch 1/10 batch 410/9741loss projected: 7.989474773406982 \n",
      "epoch 1/10 batch 412/9741loss projected: 8.135320663452148 \n",
      "epoch 1/10 batch 414/9741loss projected: 9.0320405960083 \n",
      "epoch 1/10 batch 416/9741loss projected: 9.858221054077148 \n",
      "epoch 1/10 batch 418/9741loss projected: 7.423383712768555 \n",
      "epoch 1/10 batch 420/9741loss projected: 9.161672592163086 \n",
      "epoch 1/10 batch 422/9741loss projected: 8.563714981079102 \n",
      "epoch 1/10 batch 424/9741loss projected: 7.735504150390625 \n",
      "epoch 1/10 batch 426/9741loss projected: 8.644739151000977 \n",
      "epoch 1/10 batch 428/9741loss projected: 8.814369201660156 \n",
      "epoch 1/10 batch 430/9741loss projected: 8.898200988769531 \n",
      "epoch 1/10 batch 432/9741loss projected: 8.5789213180542 \n",
      "epoch 1/10 batch 434/9741loss projected: 8.287540435791016 \n",
      "epoch 1/10 batch 436/9741loss projected: 8.864127159118652 \n",
      "epoch 1/10 batch 438/9741loss projected: 9.55733871459961 \n",
      "epoch 1/10 batch 440/9741loss projected: 8.463112831115723 \n",
      "epoch 1/10 batch 442/9741loss projected: 9.198356628417969 \n",
      "epoch 1/10 batch 444/9741loss projected: 9.154869079589844 \n",
      "epoch 1/10 batch 446/9741loss projected: 9.12471866607666 \n",
      "epoch 1/10 batch 448/9741loss projected: 8.383505821228027 \n",
      "epoch 1/10 batch 450/9741loss projected: 9.174386978149414 \n",
      "epoch 1/10 batch 452/9741loss projected: 8.811494827270508 \n",
      "epoch 1/10 batch 454/9741loss projected: 9.50722599029541 \n",
      "epoch 1/10 batch 456/9741loss projected: 6.009408473968506 \n",
      "epoch 1/10 batch 458/9741loss projected: 9.568302154541016 \n",
      "epoch 1/10 batch 460/9741loss projected: 8.272679328918457 \n",
      "epoch 1/10 batch 462/9741loss projected: 10.524234771728516 \n",
      "epoch 1/10 batch 464/9741loss projected: 10.293888092041016 \n",
      "epoch 1/10 batch 466/9741loss projected: 8.665974617004395 \n",
      "epoch 1/10 batch 468/9741loss projected: 8.15383529663086 \n",
      "epoch 1/10 batch 470/9741loss projected: 8.922176361083984 \n",
      "epoch 1/10 batch 472/9741loss projected: 9.299676895141602 \n",
      "epoch 1/10 batch 474/9741loss projected: 7.409822940826416 \n",
      "epoch 1/10 batch 476/9741loss projected: 9.277660369873047 \n",
      "epoch 1/10 batch 478/9741loss projected: 7.213582992553711 \n",
      "epoch 1/10 batch 480/9741loss projected: 7.670636177062988 \n",
      "epoch 1/10 batch 482/9741loss projected: 7.640204429626465 \n",
      "epoch 1/10 batch 484/9741loss projected: 8.668088912963867 \n",
      "epoch 1/10 batch 486/9741loss projected: 9.609596252441406 \n",
      "epoch 1/10 batch 488/9741loss projected: 8.69565200805664 \n",
      "epoch 1/10 batch 490/9741loss projected: 6.454484462738037 \n",
      "epoch 1/10 batch 492/9741loss projected: 8.39002513885498 \n",
      "epoch 1/10 batch 494/9741loss projected: 7.578168869018555 \n",
      "epoch 1/10 batch 496/9741loss projected: 8.717440605163574 \n",
      "epoch 1/10 batch 498/9741loss projected: 8.580437660217285 \n",
      "epoch 1/10 batch 500/9741loss projected: 8.166565895080566 \n",
      "epoch 1/10 batch 502/9741loss projected: 8.344597816467285 \n",
      "epoch 1/10 batch 504/9741loss projected: 9.437309265136719 \n",
      "epoch 1/10 batch 506/9741loss projected: 8.922480583190918 \n",
      "epoch 1/10 batch 508/9741loss projected: 8.510229110717773 \n",
      "epoch 1/10 batch 510/9741loss projected: 8.274264335632324 \n",
      "epoch 1/10 batch 512/9741loss projected: 9.642502784729004 \n",
      "epoch 1/10 batch 514/9741loss projected: 8.423099517822266 \n",
      "epoch 1/10 batch 516/9741loss projected: 6.270884037017822 \n",
      "epoch 1/10 batch 518/9741loss projected: 8.766241073608398 \n",
      "epoch 1/10 batch 520/9741loss projected: 8.330085754394531 \n",
      "epoch 1/10 batch 522/9741loss projected: 8.83340072631836 \n",
      "epoch 1/10 batch 524/9741loss projected: 9.21837329864502 \n",
      "epoch 1/10 batch 526/9741loss projected: 6.057285785675049 \n",
      "epoch 1/10 batch 528/9741loss projected: 7.0194010734558105 \n",
      "epoch 1/10 batch 530/9741loss projected: 7.564786911010742 \n",
      "epoch 1/10 batch 532/9741loss projected: 8.77612590789795 \n",
      "epoch 1/10 batch 534/9741loss projected: 8.183466911315918 \n",
      "epoch 1/10 batch 536/9741loss projected: 8.391124725341797 \n",
      "epoch 1/10 batch 538/9741loss projected: 8.440101623535156 \n",
      "epoch 1/10 batch 540/9741loss projected: 9.162202835083008 \n",
      "epoch 1/10 batch 542/9741loss projected: 9.19871711730957 \n",
      "epoch 1/10 batch 544/9741loss projected: 7.633413314819336 \n",
      "epoch 1/10 batch 546/9741loss projected: 9.11844539642334 \n",
      "epoch 1/10 batch 548/9741loss projected: 8.034982681274414 \n",
      "epoch 1/10 batch 550/9741loss projected: 10.336418151855469 \n",
      "epoch 1/10 batch 552/9741loss projected: 8.487476348876953 \n",
      "epoch 1/10 batch 554/9741loss projected: 8.332244873046875 \n",
      "epoch 1/10 batch 556/9741loss projected: 8.244823455810547 \n",
      "epoch 1/10 batch 558/9741loss projected: 7.503739833831787 \n",
      "epoch 1/10 batch 560/9741loss projected: 9.197405815124512 \n",
      "epoch 1/10 batch 562/9741loss projected: 5.053521156311035 \n",
      "epoch 1/10 batch 564/9741loss projected: 7.499325275421143 \n",
      "epoch 1/10 batch 566/9741loss projected: 7.906235694885254 \n",
      "epoch 1/10 batch 568/9741loss projected: 8.557568550109863 \n",
      "epoch 1/10 batch 570/9741loss projected: 5.586246490478516 \n",
      "epoch 1/10 batch 572/9741loss projected: 9.362350463867188 \n",
      "epoch 1/10 batch 574/9741loss projected: 8.363764762878418 \n",
      "epoch 1/10 batch 576/9741loss projected: 8.22160816192627 \n",
      "epoch 1/10 batch 578/9741loss projected: 9.471643447875977 \n",
      "epoch 1/10 batch 580/9741loss projected: 9.644567489624023 \n",
      "epoch 1/10 batch 582/9741loss projected: 9.301277160644531 \n",
      "epoch 1/10 batch 584/9741loss projected: 8.652466773986816 \n",
      "epoch 1/10 batch 586/9741loss projected: 7.835615634918213 \n",
      "epoch 1/10 batch 588/9741loss projected: 9.090608596801758 \n",
      "epoch 1/10 batch 590/9741loss projected: 6.586408615112305 \n",
      "epoch 1/10 batch 592/9741loss projected: 9.348542213439941 \n",
      "epoch 1/10 batch 594/9741loss projected: 7.772337913513184 \n",
      "epoch 1/10 batch 596/9741loss projected: 9.735614776611328 \n",
      "epoch 1/10 batch 598/9741loss projected: 9.025371551513672 \n",
      "epoch 1/10 batch 600/9741loss projected: 9.293009757995605 \n",
      "epoch 1/10 batch 602/9741loss projected: 8.315608978271484 \n",
      "epoch 1/10 batch 604/9741loss projected: 8.371430397033691 \n",
      "epoch 1/10 batch 606/9741loss projected: 8.127047538757324 \n",
      "epoch 1/10 batch 608/9741loss projected: 8.60619068145752 \n",
      "epoch 1/10 batch 610/9741loss projected: 9.511026382446289 \n",
      "epoch 1/10 batch 612/9741loss projected: 8.532499313354492 \n",
      "epoch 1/10 batch 614/9741loss projected: 8.11971664428711 \n",
      "epoch 1/10 batch 616/9741loss projected: 8.916606903076172 \n",
      "epoch 1/10 batch 618/9741loss projected: 9.042083740234375 \n",
      "epoch 1/10 batch 620/9741loss projected: 8.885890007019043 \n",
      "epoch 1/10 batch 622/9741loss projected: 8.55424690246582 \n",
      "epoch 1/10 batch 624/9741loss projected: 8.502946853637695 \n",
      "epoch 1/10 batch 626/9741loss projected: 7.57798433303833 \n",
      "epoch 1/10 batch 628/9741loss projected: 7.843984603881836 \n",
      "epoch 1/10 batch 630/9741loss projected: 7.4022698402404785 \n",
      "epoch 1/10 batch 632/9741loss projected: 7.206112384796143 \n",
      "epoch 1/10 batch 634/9741loss projected: 8.632612228393555 \n",
      "epoch 1/10 batch 636/9741loss projected: 7.821265697479248 \n",
      "epoch 1/10 batch 638/9741loss projected: 7.552430629730225 \n",
      "epoch 1/10 batch 640/9741loss projected: 8.08502197265625 \n",
      "epoch 1/10 batch 642/9741loss projected: 7.466692924499512 \n",
      "epoch 1/10 batch 644/9741loss projected: 8.297688484191895 \n",
      "epoch 1/10 batch 646/9741loss projected: 8.347618103027344 \n",
      "epoch 1/10 batch 648/9741loss projected: 8.855624198913574 \n",
      "epoch 1/10 batch 650/9741loss projected: 9.167879104614258 \n",
      "epoch 1/10 batch 652/9741loss projected: 9.6245698928833 \n",
      "epoch 1/10 batch 654/9741loss projected: 8.742127418518066 \n",
      "epoch 1/10 batch 656/9741loss projected: 8.188251495361328 \n",
      "epoch 1/10 batch 658/9741loss projected: 9.304999351501465 \n",
      "epoch 1/10 batch 660/9741loss projected: 7.694085597991943 \n",
      "epoch 1/10 batch 662/9741loss projected: 10.490517616271973 \n",
      "epoch 1/10 batch 664/9741loss projected: 6.2388691902160645 \n",
      "epoch 1/10 batch 666/9741loss projected: 7.428016185760498 \n",
      "epoch 1/10 batch 668/9741loss projected: 8.011434555053711 \n",
      "epoch 1/10 batch 670/9741loss projected: 6.151098251342773 \n",
      "epoch 1/10 batch 672/9741loss projected: 8.101391792297363 \n",
      "epoch 1/10 batch 674/9741loss projected: 8.57181453704834 \n",
      "epoch 1/10 batch 676/9741loss projected: 9.131425857543945 \n",
      "epoch 1/10 batch 678/9741loss projected: 8.207018852233887 \n",
      "epoch 1/10 batch 680/9741loss projected: 10.271491050720215 \n",
      "epoch 1/10 batch 682/9741loss projected: 8.075201988220215 \n",
      "epoch 1/10 batch 684/9741loss projected: 8.85331916809082 \n",
      "epoch 1/10 batch 686/9741loss projected: 7.960831165313721 \n",
      "epoch 1/10 batch 688/9741loss projected: 9.556296348571777 \n",
      "epoch 1/10 batch 690/9741loss projected: 9.397529602050781 \n",
      "epoch 1/10 batch 692/9741loss projected: 9.421226501464844 \n",
      "epoch 1/10 batch 694/9741loss projected: 8.63436222076416 \n",
      "epoch 1/10 batch 696/9741loss projected: 8.411711692810059 \n",
      "epoch 1/10 batch 698/9741loss projected: 8.472807884216309 \n",
      "epoch 1/10 batch 700/9741loss projected: 8.017457962036133 \n",
      "epoch 1/10 batch 702/9741loss projected: 7.471782207489014 \n",
      "epoch 1/10 batch 704/9741loss projected: 6.642796039581299 \n",
      "epoch 1/10 batch 706/9741loss projected: 8.54478645324707 \n",
      "epoch 1/10 batch 708/9741loss projected: 6.656879901885986 \n",
      "epoch 1/10 batch 710/9741loss projected: 6.719056606292725 \n",
      "epoch 1/10 batch 712/9741loss projected: 7.752379894256592 \n",
      "epoch 1/10 batch 714/9741loss projected: 6.59182596206665 \n",
      "epoch 1/10 batch 716/9741loss projected: 8.582955360412598 \n",
      "epoch 1/10 batch 718/9741loss projected: 8.667694091796875 \n",
      "epoch 1/10 batch 720/9741loss projected: 7.949096202850342 \n",
      "epoch 1/10 batch 722/9741loss projected: 9.31905746459961 \n",
      "epoch 1/10 batch 724/9741loss projected: 9.360063552856445 \n",
      "epoch 1/10 batch 726/9741loss projected: 8.200972557067871 \n",
      "epoch 1/10 batch 728/9741loss projected: 9.890056610107422 \n",
      "epoch 1/10 batch 730/9741loss projected: 8.741046905517578 \n",
      "epoch 1/10 batch 732/9741loss projected: 9.438435554504395 \n",
      "epoch 1/10 batch 734/9741loss projected: 9.025527954101562 \n",
      "epoch 1/10 batch 736/9741loss projected: 10.012591361999512 \n",
      "epoch 1/10 batch 738/9741loss projected: 9.430419921875 \n",
      "epoch 1/10 batch 740/9741loss projected: 8.264801025390625 \n",
      "epoch 1/10 batch 742/9741loss projected: 9.017538070678711 \n",
      "epoch 1/10 batch 744/9741loss projected: 8.573450088500977 \n",
      "epoch 1/10 batch 746/9741loss projected: 8.339226722717285 \n",
      "epoch 1/10 batch 748/9741loss projected: 9.917603492736816 \n",
      "epoch 1/10 batch 750/9741loss projected: 8.374100685119629 \n",
      "epoch 1/10 batch 752/9741loss projected: 6.445300579071045 \n",
      "epoch 1/10 batch 754/9741loss projected: 7.698973655700684 \n",
      "epoch 1/10 batch 756/9741loss projected: 10.535399436950684 \n",
      "epoch 1/10 batch 758/9741loss projected: 9.48146915435791 \n",
      "epoch 1/10 batch 760/9741loss projected: 8.882882118225098 \n",
      "epoch 1/10 batch 762/9741loss projected: 8.005300521850586 \n",
      "epoch 1/10 batch 764/9741loss projected: 10.409709930419922 \n",
      "epoch 1/10 batch 766/9741loss projected: 9.003937721252441 \n",
      "epoch 1/10 batch 768/9741loss projected: 7.477960586547852 \n",
      "epoch 1/10 batch 770/9741loss projected: 7.257047653198242 \n",
      "epoch 1/10 batch 772/9741loss projected: 8.471311569213867 \n",
      "epoch 1/10 batch 774/9741loss projected: 8.997749328613281 \n",
      "epoch 1/10 batch 776/9741loss projected: 10.068557739257812 \n",
      "epoch 1/10 batch 778/9741loss projected: 7.873000144958496 \n",
      "epoch 1/10 batch 780/9741loss projected: 8.235018730163574 \n",
      "epoch 1/10 batch 782/9741loss projected: 9.296536445617676 \n",
      "epoch 1/10 batch 784/9741loss projected: 8.532544136047363 \n",
      "epoch 1/10 batch 786/9741loss projected: 7.3670268058776855 \n",
      "epoch 1/10 batch 788/9741loss projected: 7.724435329437256 \n",
      "epoch 1/10 batch 790/9741loss projected: 8.650717735290527 \n",
      "epoch 1/10 batch 792/9741loss projected: 8.965032577514648 \n",
      "epoch 1/10 batch 794/9741loss projected: 9.172558784484863 \n",
      "epoch 1/10 batch 796/9741loss projected: 10.389189720153809 \n",
      "epoch 1/10 batch 798/9741loss projected: 10.2374906539917 \n",
      "epoch 1/10 batch 800/9741loss projected: 8.643111228942871 \n",
      "epoch 1/10 batch 802/9741loss projected: 6.838733673095703 \n",
      "epoch 1/10 batch 804/9741loss projected: 7.411044597625732 \n",
      "epoch 1/10 batch 806/9741loss projected: 8.978433609008789 \n",
      "epoch 1/10 batch 808/9741loss projected: 9.3643217086792 \n",
      "epoch 1/10 batch 810/9741loss projected: 8.931442260742188 \n",
      "epoch 1/10 batch 812/9741loss projected: 8.054303169250488 \n",
      "epoch 1/10 batch 814/9741loss projected: 8.728364944458008 \n",
      "epoch 1/10 batch 816/9741loss projected: 8.007610321044922 \n",
      "epoch 1/10 batch 818/9741loss projected: 7.808478355407715 \n",
      "epoch 1/10 batch 820/9741loss projected: 9.555420875549316 \n",
      "epoch 1/10 batch 822/9741loss projected: 9.459609031677246 \n",
      "epoch 1/10 batch 824/9741loss projected: 9.072154998779297 \n",
      "epoch 1/10 batch 826/9741loss projected: 8.111266136169434 \n",
      "epoch 1/10 batch 828/9741loss projected: 9.227082252502441 \n",
      "epoch 1/10 batch 830/9741loss projected: 9.352825164794922 \n",
      "epoch 1/10 batch 832/9741loss projected: 7.748006343841553 \n",
      "epoch 1/10 batch 834/9741loss projected: 6.633682727813721 \n",
      "epoch 1/10 batch 836/9741loss projected: 7.456009864807129 \n",
      "epoch 1/10 batch 838/9741loss projected: 7.572546005249023 \n",
      "epoch 1/10 batch 840/9741loss projected: 9.44494342803955 \n",
      "epoch 1/10 batch 842/9741loss projected: 8.56661605834961 \n",
      "epoch 1/10 batch 844/9741loss projected: 8.43347454071045 \n",
      "epoch 1/10 batch 846/9741loss projected: 9.3562593460083 \n",
      "epoch 1/10 batch 848/9741loss projected: 8.748041152954102 \n",
      "epoch 1/10 batch 850/9741loss projected: 6.237166404724121 \n",
      "epoch 1/10 batch 852/9741loss projected: 8.625267028808594 \n",
      "epoch 1/10 batch 854/9741loss projected: 9.094927787780762 \n",
      "epoch 1/10 batch 856/9741loss projected: 7.276431560516357 \n",
      "epoch 1/10 batch 858/9741loss projected: 9.145699501037598 \n",
      "epoch 1/10 batch 860/9741loss projected: 7.995895862579346 \n",
      "epoch 1/10 batch 862/9741loss projected: 9.449190139770508 \n",
      "epoch 1/10 batch 864/9741loss projected: 10.448740005493164 \n",
      "epoch 1/10 batch 866/9741loss projected: 8.158778190612793 \n",
      "epoch 1/10 batch 868/9741loss projected: 8.428942680358887 \n",
      "epoch 1/10 batch 870/9741loss projected: 7.2349371910095215 \n",
      "epoch 1/10 batch 872/9741loss projected: 8.068897247314453 \n",
      "epoch 1/10 batch 874/9741loss projected: 6.437851905822754 \n",
      "epoch 1/10 batch 876/9741loss projected: 8.823386192321777 \n",
      "epoch 1/10 batch 878/9741loss projected: 9.019987106323242 \n",
      "epoch 1/10 batch 880/9741loss projected: 8.382978439331055 \n",
      "epoch 1/10 batch 882/9741loss projected: 11.157800674438477 \n",
      "epoch 1/10 batch 884/9741loss projected: 7.240501403808594 \n",
      "epoch 1/10 batch 886/9741loss projected: 8.734074592590332 \n",
      "epoch 1/10 batch 888/9741loss projected: 9.050868034362793 \n",
      "epoch 1/10 batch 890/9741loss projected: 7.03912878036499 \n",
      "epoch 1/10 batch 892/9741loss projected: 8.818757057189941 \n",
      "epoch 1/10 batch 894/9741loss projected: 6.887528896331787 \n",
      "epoch 1/10 batch 896/9741loss projected: 9.464348793029785 \n",
      "epoch 1/10 batch 898/9741loss projected: 8.609034538269043 \n",
      "epoch 1/10 batch 900/9741loss projected: 8.723410606384277 \n",
      "epoch 1/10 batch 902/9741loss projected: 9.288923263549805 \n",
      "epoch 1/10 batch 904/9741loss projected: 8.967272758483887 \n",
      "epoch 1/10 batch 906/9741loss projected: 6.296663761138916 \n",
      "epoch 1/10 batch 908/9741loss projected: 9.05974292755127 \n",
      "epoch 1/10 batch 910/9741loss projected: 9.291696548461914 \n",
      "epoch 1/10 batch 912/9741loss projected: 8.670476913452148 \n",
      "epoch 1/10 batch 914/9741loss projected: 8.166959762573242 \n",
      "epoch 1/10 batch 916/9741loss projected: 8.151877403259277 \n",
      "epoch 1/10 batch 918/9741loss projected: 9.654715538024902 \n",
      "epoch 1/10 batch 920/9741loss projected: 9.76554012298584 \n",
      "epoch 1/10 batch 922/9741loss projected: 8.525352478027344 \n",
      "epoch 1/10 batch 924/9741loss projected: 8.082810401916504 \n",
      "epoch 1/10 batch 926/9741loss projected: 6.547264099121094 \n",
      "epoch 1/10 batch 928/9741loss projected: 9.166932106018066 \n",
      "epoch 1/10 batch 930/9741loss projected: 8.897159576416016 \n",
      "epoch 1/10 batch 932/9741loss projected: 9.055121421813965 \n",
      "epoch 1/10 batch 934/9741loss projected: 9.711566925048828 \n",
      "epoch 1/10 batch 936/9741loss projected: 9.578532218933105 \n",
      "epoch 1/10 batch 938/9741loss projected: 8.059677124023438 \n",
      "epoch 1/10 batch 940/9741loss projected: 8.76609992980957 \n",
      "epoch 1/10 batch 942/9741loss projected: 8.564798355102539 \n",
      "epoch 1/10 batch 944/9741loss projected: 8.396063804626465 \n",
      "epoch 1/10 batch 946/9741loss projected: 9.062518119812012 \n",
      "epoch 1/10 batch 948/9741loss projected: 9.081979751586914 \n",
      "epoch 1/10 batch 950/9741loss projected: 8.775671005249023 \n",
      "epoch 1/10 batch 952/9741loss projected: 9.263105392456055 \n",
      "epoch 1/10 batch 954/9741loss projected: 8.17301082611084 \n",
      "epoch 1/10 batch 956/9741loss projected: 8.81945514678955 \n",
      "epoch 1/10 batch 958/9741loss projected: 9.807473182678223 \n",
      "epoch 1/10 batch 960/9741loss projected: 7.952308654785156 \n",
      "epoch 1/10 batch 962/9741loss projected: 9.279658317565918 \n",
      "epoch 1/10 batch 964/9741loss projected: 8.578071594238281 \n",
      "epoch 1/10 batch 966/9741loss projected: 9.402785301208496 \n",
      "epoch 1/10 batch 968/9741loss projected: 9.001346588134766 \n",
      "epoch 1/10 batch 970/9741loss projected: 9.398075103759766 \n",
      "epoch 1/10 batch 972/9741loss projected: 6.416728496551514 \n",
      "epoch 1/10 batch 974/9741loss projected: 8.57343578338623 \n",
      "epoch 1/10 batch 976/9741loss projected: 9.647377967834473 \n",
      "epoch 1/10 batch 978/9741loss projected: 8.485270500183105 \n",
      "epoch 1/10 batch 980/9741loss projected: 9.506233215332031 \n",
      "epoch 1/10 batch 982/9741loss projected: 9.08556842803955 \n",
      "epoch 1/10 batch 984/9741loss projected: 8.657065391540527 \n",
      "epoch 1/10 batch 986/9741loss projected: 7.97905969619751 \n",
      "epoch 1/10 batch 988/9741loss projected: 8.636232376098633 \n",
      "epoch 1/10 batch 990/9741loss projected: 8.723001480102539 \n",
      "epoch 1/10 batch 992/9741loss projected: 8.065364837646484 \n",
      "epoch 1/10 batch 994/9741loss projected: 9.350152969360352 \n",
      "epoch 1/10 batch 996/9741loss projected: 9.04250717163086 \n",
      "epoch 1/10 batch 998/9741loss projected: 9.039936065673828 \n",
      "epoch 1/10 batch 1000/9741loss projected: 8.460309028625488 \n",
      "epoch 1/10 batch 1002/9741loss projected: 10.315049171447754 \n",
      "epoch 1/10 batch 1004/9741loss projected: 8.433110237121582 \n",
      "epoch 1/10 batch 1006/9741loss projected: 10.275520324707031 \n",
      "epoch 1/10 batch 1008/9741loss projected: 8.371575355529785 \n",
      "epoch 1/10 batch 1010/9741loss projected: 10.413361549377441 \n",
      "epoch 1/10 batch 1012/9741loss projected: 8.585286140441895 \n",
      "epoch 1/10 batch 1014/9741loss projected: 9.375304222106934 \n",
      "epoch 1/10 batch 1016/9741loss projected: 8.181909561157227 \n",
      "epoch 1/10 batch 1018/9741loss projected: 7.383487224578857 \n",
      "epoch 1/10 batch 1020/9741loss projected: 9.234533309936523 \n",
      "epoch 1/10 batch 1022/9741loss projected: 9.436311721801758 \n",
      "epoch 1/10 batch 1024/9741loss projected: 8.48011302947998 \n",
      "epoch 1/10 batch 1026/9741loss projected: 9.244255065917969 \n",
      "epoch 1/10 batch 1028/9741loss projected: 13.008587837219238 \n",
      "epoch 1/10 batch 1030/9741loss projected: 9.771142959594727 \n",
      "epoch 1/10 batch 1032/9741loss projected: 8.88247013092041 \n",
      "epoch 1/10 batch 1034/9741loss projected: 9.16720199584961 \n",
      "epoch 1/10 batch 1036/9741loss projected: 7.9411115646362305 \n",
      "epoch 1/10 batch 1038/9741loss projected: 8.939611434936523 \n",
      "epoch 1/10 batch 1040/9741loss projected: 8.585206031799316 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (150x1049 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m input_embeddings \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(input_embeddings, (\u001b[39m0\u001b[39m, padding_size), \u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m input_embeddings_projected \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(input_embeddings)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m weights \u001b[39m=\u001b[39m learn_weights(input_embeddings)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m projected_prompt_batch \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mexpand_as(basis) \u001b[39m*\u001b[39m basis\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m projected_prompt_batch \u001b[39m=\u001b[39m projected_prompt_batch\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(batch_size, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiments/experiment_10/experiment_10.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x))\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/nlp_project/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (150x1049 and 1024x512)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "\n",
    "# Initialize the tokenizer and models (one or continuous prompting and other for projected prompting\n",
    "model_projected = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_projected.to(device)\n",
    "\n",
    "# Define the prompt basis\n",
    "prompt_list = [\n",
    "    'When you see the following question, I would like you to answer it correctly',\n",
    "    'Produce an executable artifact of type X that will answer the question, and then execute it',\n",
    "    'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When you then answered the three questions, combine the answers to produce the final answers to my original question',\n",
    "    'Generate a set of facts that are contained in the output. The set of facts should be inserted in a specific point in the output to answer the question',\n",
    "    'Given the following question, generate a detailed explanation before providing the correct answer',\n",
    "    'Imagine you are a teacher explaining the answer to this question to a student. How would you respond?',\n",
    "    'Consider the following question. What are the key concepts involved and how do they lead to the correct answer?',\n",
    "    'As an expert in the field, how would you respond to the following question?',\n",
    "    'Translate the following question into a simpler form, then provide the answer',\n",
    "    'If you were to create a diagram to answer this question, what would it look like? Describe it in detail',\n",
    "    'Pretend you are explaining the answer to this question to someone with no background in the subject. How would you explain it?',\n",
    "    'As a highly proficient translator, translate the following question into a different context, then provide the answer',\n",
    "    'Generate a step-by-step guide to answer the following question',\n",
    "    'Consider the following question. What assumptions are you making in order to answer it?',\n",
    "    'If you were to debate the answer to this question, what points would you raise?',\n",
    "    'As a researcher, how would you investigate the answer to the following question?',\n",
    "    'Pretend you are a journalist reporting on the answer to this question. How would you present it?',\n",
    "    'As a storyteller, weave a narrative around the answer to this question',\n",
    "    'If you were to answer this question in a court of law, what evidence would you present?',\n",
    "    'As a detective, how would you piece together the answer to this question?',\n",
    "    'Imagine you are a computer program designed to answer this question. What algorithms or processes would you use?',\n",
    "    'As a philosopher, how would you interpret the answer to this question?',\n",
    "    'If you were to answer this question in a job interview, how would you respond?',\n",
    "    'As a scientist, how would you experiment to find the answer to this question?'\n",
    "]\n",
    "\n",
    "print(f'tokenizing prompts')\n",
    "print(f'prompt list length {len(prompt_list)}')\n",
    "\n",
    "basis = tokenizer(prompt_list, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "basis = model_projected.model.shared(basis.input_ids)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['inputs_pretokenized'], truncation=True, padding='max_length')\n",
    "\n",
    "# Apply the function to the dataset\n",
    "print('tokenzing dataset')\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the weight prediction model\n",
    "class LearnWeights(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LearnWeights, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = x.mean(dim=1, keepdim=True)  # Compute the mean across the token dimension and batch dimension\n",
    "        return x.squeeze(1).mean(dim=0)\n",
    "\n",
    "# Define the projected prompt\n",
    "input_dim = 1024\n",
    "\n",
    "output_dim = len(prompt_list)\n",
    "learn_weights = LearnWeights(input_dim, output_dim).to(device)\n",
    "optimizer_projected = AdamW(learn_weights.parameters())\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('starting training')\n",
    "\n",
    "# Training loop\n",
    "projected_losses = []\n",
    "val_losses = []\n",
    "\n",
    "shapes = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_continuous = 0\n",
    "    epoch_loss_projected = 0\n",
    "    for i in range(0, len(train_dataset)-8700, batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_projected.model.shared(input_ids)\n",
    "        padding_size = max(0, 100 - input_embeddings.shape[1])\n",
    "        input_embeddings = F.pad(input_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "\n",
    "        \n",
    "        weights = learn_weights(input_embeddings)\n",
    "        # print(f'predicted weights' + str(weights))\n",
    "        # print(f'predicted weights shape' + str(weights.shape))\n",
    "        # print(f'basis shape' + str(basis.shape))\n",
    "        # print(f'input embeddings shape' + str(input_embeddings.shape))\n",
    "        # print(f'soft prompt shape' + str(soft_prompt.shape))\n",
    "        # print(f'soft prompt batch shape' + str(soft_prompt_batch.shape))\n",
    "        projected_prompt_batch = weights.unsqueeze(1).unsqueeze(2).expand_as(basis) * basis\n",
    "        projected_prompt_batch = projected_prompt_batch.sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        # print(f'projected prompt batch shape' + str(projected_prompt_batch.shape))\n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "        \n",
    "    \n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "\n",
    "        loss_projected = outputs_projected.loss\n",
    "        epoch_loss_projected += loss_projected.item()\n",
    "\n",
    "        optimizer_projected.zero_grad()\n",
    "        loss_projected.backward(retain_graph=True)\n",
    "        optimizer_projected.step()\n",
    "\n",
    "        #print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'epoch {epoch+1}/{epochs} batch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'loss projected: {loss_projected.item()} \\n', end='')\n",
    "    \n",
    "        # Validation loop\n",
    "    val_loss = 0\n",
    "    model_projected.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(0, len(val_dataset), batch_size):\n",
    "            batch = val_dataset[i:i+batch_size]\n",
    "            input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "            labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "            input_embeddings = model_projected.model.shared(input_ids)\n",
    "            padding_size = max(0, 100 - input_embeddings.shape[1])\n",
    "            input_embeddings = F.pad(input_embeddings, (0, padding_size), \"constant\", 0)\n",
    "            input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "            weights = learn_weights(input_embeddings)\n",
    "            projected_prompt_batch = weights.unsqueeze(1).unsqueeze(2).expand_as(basis) * basis\n",
    "            projected_prompt_batch = projected_prompt_batch.sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "            combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "            outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "            val_loss += outputs_projected.loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_losses.append(val_loss)    \n",
    "\n",
    "    epoch_loss_continuous /= len(train_dataset)\n",
    "    epoch_loss_projected /= len(train_dataset)\n",
    "\n",
    "    projected_losses.append(epoch_loss_projected)\n",
    "\n",
    "    # Create a DataFrame with the loss values\n",
    "    n = len(projected_losses)\n",
    "    data = {\n",
    "        'Epoch': list(range(1, n + 1)),\n",
    "        'Loss': projected_losses,\n",
    "        'Model': ['Projected'] * n\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df_val = pd.DataFrame({\n",
    "    'Epoch': list(range(1, len(val_losses) + 1)),\n",
    "    'Loss': val_losses,\n",
    "    'Model': ['Validation'] * len(val_losses)\n",
    "    })\n",
    "    df = pd.concat([df, df_val])\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Epoch', y='Loss', hue='Model')\n",
    "    plt.title('Loss per Epoch for Continuous and Projected Models')\n",
    "\n",
    "    # Save the plot as an SVG file\n",
    "    plt.savefig(f'/Users/Pascal/Projects/nlp_project/experiments/experiment_10/loss_plot_epoch_{epoch+1}.svg', format='svg')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
