{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from seaborn) (1.23.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.0.9)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: requests in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: pandas in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (4.37.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from matplotlib) (1.23.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: protobuf==3.20 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (3.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.5/386.5 kB\u001b[0m \u001b[31m864.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (1.23.3)\n",
      "Collecting pyarrow-hotfix\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (0.19.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m755.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: packaging in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.3-cp310-cp310-macosx_11_0_arm64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: filelock in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Pascal/.julia/conda/3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, attrs, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.1\n",
      "    Uninstalling fsspec-2023.12.1:\n",
      "      Successfully uninstalled fsspec-2023.12.1\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 datasets-2.15.0 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.10.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%pip install transformers \n",
    "%pip install matplotlib pandas\n",
    "%pip install protobuf==3.20\n",
    "%pip install torch\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "starting training\n",
      "input embeddings shape: torch.Size([4, 82, 1024])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 1, but got 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Pascal/Projects/nlp_project/experiment_5.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m soft_prompt_batch \u001b[39m=\u001b[39m soft_prompt\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(batch_size, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# print(f'soft prompt shape: {soft_prompt.shape}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m#the goal is to have good initizlization of the soft prompt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39m# add copies of projected prompt for each element in the batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m weights \u001b[39m=\u001b[39m learn_weights(basis)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m projected_prompt_batch \u001b[39m=\u001b[39m (weights \u001b[39m*\u001b[39m basis)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(batch_size, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\u001b[39;00m\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/Pascal/Projects/nlp_project/experiment_5.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(x, x, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Pascal/Projects/nlp_project/experiment_5.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39msoftmax(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m)\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.julia/conda/3/lib/python3.10/site-packages/torch/nn/functional.py:5280\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5274\u001b[0m     \u001b[39mif\u001b[39;00m key_padding_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5275\u001b[0m         \u001b[39m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[1;32m   5276\u001b[0m         \u001b[39m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[1;32m   5277\u001b[0m         \u001b[39m# longer causal.\u001b[39;00m\n\u001b[1;32m   5278\u001b[0m         is_causal \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 5280\u001b[0m \u001b[39massert\u001b[39;00m embed_dim \u001b[39m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5281\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwas expecting embedding dimension of \u001b[39m\u001b[39m{\u001b[39;00membed_dim_to_check\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00membed_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embed_dim, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   5283\u001b[0m     \u001b[39m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5284\u001b[0m     head_dim \u001b[39m=\u001b[39m embed_dim\u001b[39m.\u001b[39mdiv(num_heads, rounding_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrunc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 1, but got 1024"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Initialize the tokenizer and models (one or continuous prompting and other for projected prompting\n",
    "model_continuous = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "model_projected = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_continuous.to(device)\n",
    "model_projected.to(device)\n",
    "\n",
    "# Define the prompt basis\n",
    "prompt_list = [\n",
    "    'When you see the following question, I would like you to answer it correctly',\n",
    "    'Produce an executable artifact of type X that will answer the question, and then execute it',\n",
    "    'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When you then answered the three questions, combine the answers to produce the final answers to my original question  ',\n",
    "    'Generate a set of facts that are contained in the output. The set of facts should be inserted in a specific point in the output to answer the question',\n",
    "]\n",
    "\n",
    "print(f'tokenizing')\n",
    "\n",
    "basis = tokenizer(prompt_list, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "basis = model_projected.model.shared(basis.input_ids)\n",
    "\n",
    "# print(f'prompt basis shape: {basis.shape}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the weight prediction model\n",
    "class LearnWeights(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LearnWeights, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)  # Change this line\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=-1).unsqueeze(-1).repeat(1, 1, 1024)\n",
    "\n",
    "# Define the soft prompt\n",
    "# we want good initialization of the soft prompt\n",
    "soft_prompt = torch.mean(basis, dim=0)\n",
    "soft_prompt = torch.nn.Parameter(soft_prompt)\n",
    "optimizer_continuous = AdamW([soft_prompt])\n",
    "\n",
    "# Define the projected prompt\n",
    "input_dim = 1\n",
    "hidden_dim = 1\n",
    "output_dim = len(prompt_list)\n",
    "learn_weights = LearnWeights(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer_projected = AdamW(learn_weights.parameters())\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1\n",
    "batch_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('starting training')\n",
    "\n",
    "# Training loop\n",
    "continuous_losses = []\n",
    "projected_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_continuous = 0\n",
    "    epoch_loss_projected = 0\n",
    "    for i in range(0, len(train_dataset) - 9000, batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_continuous.model.shared(input_ids)\n",
    "        \n",
    "        print(f'input embeddings shape: {input_embeddings.shape}')\n",
    "\n",
    "        # add copies of soft prompt for each element in the batch\n",
    "        soft_prompt_batch = soft_prompt.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        # print(f'soft prompt shape: {soft_prompt.shape}')\n",
    "        #the goal is to have good initizlization of the soft prompt\n",
    "        \n",
    "        # add copies of projected prompt for each element in the batch\n",
    "        weights = learn_weights(basis)\n",
    "        projected_prompt_batch = (weights * basis).sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "        \n",
    "        combined_continuous_embeddings = torch.cat([soft_prompt_batch, input_embeddings], dim=1)\n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings], dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs_continuous = model_continuous(inputs_embeds=combined_continuous_embeddings, labels=labels)\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "\n",
    "        loss_continuous = outputs_continuous.loss\n",
    "        epoch_loss_continuous += loss_continuous.item()\n",
    "\n",
    "        loss_projected = outputs_projected.loss\n",
    "        epoch_loss_projected += loss_projected.item()\n",
    "\n",
    "        optimizer_continuous.zero_grad()\n",
    "        loss_continuous.backward(retain_graph=True)\n",
    "        optimizer_continuous.step()\n",
    "\n",
    "        optimizer_continuous.zero_grad()\n",
    "        loss_projected.backward()\n",
    "        optimizer_continuous.step()\n",
    "\n",
    "        print(f'\\r complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'\\r loss continuous: {loss_continuous.item()}', end='')\n",
    "        print(f'\\r loss projected: {loss_projected.item()}', end='')\n",
    "\n",
    "    epoch_loss_continuous /= len(train_dataset)\n",
    "    epoch_loss_projected /= len(train_dataset)\n",
    "\n",
    "    continuous_losses.append(epoch_loss_continuous)\n",
    "    projected_losses.append(epoch_loss_projected)\n",
    "\n",
    "    print(f'\\r Epoch {epoch+1}/{epochs} complete. Loss: {epoch_loss_continuous}')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame with the loss values\n",
    "data = {\n",
    "    'Epoch': list(range(1, epochs + 1)) * 2,\n",
    "    'Loss': continuous_losses + projected_losses,\n",
    "    'Model': ['Continuous'] * epochs + ['Projected'] * epochs\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Epoch', y='Loss', hue='Model')\n",
    "plt.title('Loss per Epoch for Continuous and Projected Models')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
