{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "starting training\n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppxscal/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss continuous: 8.530298233032227loss projected: 8.442331314086914 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.628532409667969loss projected: 7.9915642738342285 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.976092338562012loss projected: 9.126500129699707 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.203394412994385loss projected: 8.1417236328125 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.321535110473633loss projected: 7.636289596557617 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.427311420440674loss projected: 7.79829740524292 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 9.357646942138672loss projected: 9.799015045166016 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.037398338317871loss projected: 8.171722412109375 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.299705505371094loss projected: 8.196093559265137 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.268082618713379loss projected: 8.68516731262207 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.738925457000732loss projected: 8.652424812316895 \n",
      " Epoch 1/3 complete. Loss: 0.009012311137122744\n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.201757431030273loss projected: 8.442331314086914 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 7.132340431213379loss projected: 7.9915642738342285 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.552399635314941loss projected: 9.126500129699707 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 6.81311559677124loss projected: 8.1417236328125 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 6.806525707244873loss projected: 7.636289596557617 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 6.981721878051758loss projected: 7.79829740524292 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "loss continuous: 8.795413970947266loss projected: 9.799015045166016 \n",
      "predicted weightstensor([[[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]],\n",
      "\n",
      "        [[0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         [0.0233, 0.0233, 0.0233,  ..., 0.0233, 0.0233, 0.0233],\n",
      "         ...,\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232],\n",
      "         [0.0232, 0.0232, 0.0232,  ..., 0.0232, 0.0232, 0.0232]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m optimizer_continuous\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m optimizer_continuous\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m loss_projected\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m optimizer_continuous\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/experiment_7.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m#print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Initialize the tokenizer and models (one or continuous prompting and other for projected prompting\n",
    "model_continuous = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "model_projected = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_continuous.to(device)\n",
    "model_projected.to(device)\n",
    "\n",
    "# Define the prompt basis\n",
    "prompt_list = [\n",
    "    'When you see the following question, I would like you to answer it correctly',\n",
    "    'Produce an executable artifact of type X that will answer the question, and then execute it',\n",
    "    'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When you then answered the three questions, combine the answers to produce the final answers to my original question',\n",
    "    'Generate a set of facts that are contained in the output. The set of facts should be inserted in a specific point in the output to answer the question',\n",
    "    'Given the following question, generate a detailed explanation before providing the correct answer',\n",
    "    'Imagine you are a teacher explaining the answer to this question to a student. How would you respond?',\n",
    "    'Consider the following question. What are the key concepts involved and how do they lead to the correct answer?',\n",
    "    'As an expert in the field, how would you respond to the following question?',\n",
    "    'Translate the following question into a simpler form, then provide the answer',\n",
    "    'If you were to create a diagram to answer this question, what would it look like? Describe it in detail',\n",
    "    'Pretend you are explaining the answer to this question to someone with no background in the subject. How would you explain it?',\n",
    "    'As a highly proficient translator, translate the following question into a different context, then provide the answer',\n",
    "    'Generate a step-by-step guide to answer the following question',\n",
    "    'Consider the following question. What assumptions are you making in order to answer it?',\n",
    "    'If you were to debate the answer to this question, what points would you raise?',\n",
    "    'As a researcher, how would you investigate the answer to the following question?',\n",
    "    'Pretend you are a journalist reporting on the answer to this question. How would you present it?',\n",
    "    'As a storyteller, weave a narrative around the answer to this question',\n",
    "    'If you were to answer this question in a court of law, what evidence would you present?',\n",
    "    'As a detective, how would you piece together the answer to this question?',\n",
    "    'Imagine you are a computer program designed to answer this question. What algorithms or processes would you use?',\n",
    "    'As a philosopher, how would you interpret the answer to this question?',\n",
    "    'If you were to answer this question in a job interview, how would you respond?',\n",
    "    'As a scientist, how would you experiment to find the answer to this question?'\n",
    "]\n",
    "\n",
    "print(f'tokenizing')\n",
    "\n",
    "basis = tokenizer(prompt_list, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "basis = model_projected.model.shared(basis.input_ids)\n",
    "\n",
    "# print(f'prompt basis shape: {basis.shape}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the weight prediction model\n",
    "class LearnWeights(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LearnWeights, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, hidden_dim) \n",
    "        self.fc = nn.Linear(1024, output_dim) # Change input dimension to 1024\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.attention(x, x, x) \n",
    "        x = self.fc(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2) # Add global average pooling\n",
    "        return F.softmax(x, dim=-1).unsqueeze(-1).repeat(1, 1, 1024)\n",
    "\n",
    "# Define the soft prompt\n",
    "# we want good initialization of the soft prompt\n",
    "soft_prompt = torch.mean(basis, dim=0)\n",
    "soft_prompt = torch.nn.Parameter(soft_prompt)\n",
    "optimizer_continuous = AdamW([soft_prompt])\n",
    "\n",
    "# Define the projected prompt\n",
    "input_dim = 1024\n",
    "hidden_dim = 128\n",
    "output_dim = len(prompt_list)\n",
    "learn_weights = LearnWeights(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer_projected = AdamW(learn_weights.parameters())\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "batch_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('starting training')\n",
    "\n",
    "# Training loop\n",
    "continuous_losses = []\n",
    "projected_losses = []\n",
    "\n",
    "shapes = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_continuous = 0\n",
    "    epoch_loss_projected = 0\n",
    "    for i in range(0, len(train_dataset) - 9700, batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # Get the prompt input embeddings - same if continuous or projected\n",
    "        input_embeddings = model_continuous.model.shared(input_ids)\n",
    "        padding_size = max(0, 100 - input_embeddings.shape[1])\n",
    "        input_embeddings = F.pad(input_embeddings, (0, 0, 0, padding_size), \"constant\", 0)\n",
    "        input_embeddings_projected = torch.Tensor(input_embeddings).to(device)\n",
    "        \n",
    "        #print(f'input embeddings shape: {input_embeddings.shape}')\n",
    "    \n",
    "\n",
    "        # add copies of soft prompt for each element in the batch\n",
    "        soft_prompt_batch = soft_prompt.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        # print(f'soft prompt shape: {soft_prompt.shape}')\n",
    "        #the goal is to have good initizlization of the soft prompt\n",
    "        \n",
    "        # add copies of projected prompt for each element in the batch\n",
    "        #print(f'basis shape: {basis.shape}')\n",
    "        \n",
    "        weights = learn_weights(basis)\n",
    "        print(f'predicted weights' + str(weights))\n",
    "        print(f'predicted weights shape' + str(weights.shape))\n",
    "        projected_prompt_batch = (weights * basis).sum(dim=0).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        # print(f'shapes of soft batch and input embeddings: {soft_prompt_batch.shape}, {input_embeddings.shape}')\n",
    "        \n",
    "        combined_continuous_embeddings = torch.cat([soft_prompt_batch, input_embeddings], dim=1)\n",
    "        combined_projected_embeddings = torch.cat([projected_prompt_batch, input_embeddings_projected], dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs_continuous = model_continuous(inputs_embeds=combined_continuous_embeddings, labels=labels)\n",
    "        outputs_projected = model_projected(inputs_embeds=combined_projected_embeddings, labels=labels)\n",
    "\n",
    "        loss_continuous = outputs_continuous.loss\n",
    "        epoch_loss_continuous += loss_continuous.item()\n",
    "\n",
    "        loss_projected = outputs_projected.loss\n",
    "        epoch_loss_projected += loss_projected.item()\n",
    "\n",
    "        optimizer_continuous.zero_grad()\n",
    "        loss_continuous.backward(retain_graph=True)\n",
    "        optimizer_continuous.step()\n",
    "\n",
    "        optimizer_continuous.zero_grad()\n",
    "        loss_projected.backward(retain_graph=True)\n",
    "        optimizer_continuous.step()\n",
    "\n",
    "        #print(f'complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'loss continuous: {loss_continuous.item()}', end='')\n",
    "        print(f'loss projected: {loss_projected.item()} \\n', end='')\n",
    "\n",
    "    epoch_loss_continuous /= len(train_dataset)\n",
    "    epoch_loss_projected /= len(train_dataset)\n",
    "\n",
    "    continuous_losses.append(epoch_loss_continuous)\n",
    "    projected_losses.append(epoch_loss_projected)\n",
    "\n",
    "    print(f'\\r Epoch {epoch+1}/{epochs} complete. Loss: {epoch_loss_continuous}')\n",
    "\n",
    "\n",
    "# Create a DataFrame with the loss values\n",
    "data = {\n",
    "    'Epoch': list(range(1, epochs + 1)) * 2,\n",
    "    'Loss': continuous_losses + projected_losses,\n",
    "    'Model': ['Continuous'] * epochs + ['Projected'] * epochs\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Epoch', y='Loss', hue='Model')\n",
    "plt.title('Loss per Epoch for Continuous and Projected Models')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
