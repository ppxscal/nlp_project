{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ppxscal/.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /home/ppxscal/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: peft in /home/ppxscal/.local/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: protobuf==3.20 in /home/ppxscal/.local/lib/python3.10/site-packages (3.20.0)\n",
      "Requirement already satisfied: requests in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: filelock in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pandas in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/ppxscal/.local/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: psutil in /home/ppxscal/.local/lib/python3.10/site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from peft) (0.24.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ppxscal/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ppxscal/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ppxscal/.local/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ppxscal/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: networkx in /home/ppxscal/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ppxscal/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ppxscal/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ppxscal/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ppxscal/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ppxscal/.local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets peft protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# List of dataset splits\n",
    "splits = [\"cos_e_v1.11_aligned_with_common_sense\", \"cos_e_v1.11_description_question_option_id\", \n",
    "          \"cos_e_v1.11_description_question_option_text\", \"cos_e_v1.11_explain_why_human\", \n",
    "          \"cos_e_v1.11_generate_explanation_given_text\", \"cos_e_v1.11_i_think\", \n",
    "          \"cos_e_v1.11_question_description_option_id\", \"cos_e_v1.11_question_description_option_text\", \n",
    "          \"cos_e_v1.11_question_option_description_id\", \"cos_e_v1.11_question_option_description_text\", \n",
    "          \"cos_e_v1.11_rationale\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, TaskType, get_peft_model\n",
    "#load the model and tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt_tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    num_virtual_tokens=10\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, prompt_tuning_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from cos_e_v1.11_aligned_with_common_sense:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'inputs': [[947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 29403, 71, 10, 2447, 6, 7089, 484, 6, 2004, 1530, 6, 7270, 682, 6, 18076, 1615, 19, 96, 3357, 107, 682, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 29403, 71, 10, 2601, 14089, 6, 2608, 6, 2412, 2478, 6, 4716, 6, 4716, 1615, 19, 96, 9818, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 29403, 71, 10, 1287, 6, 23868, 6, 3331, 6, 11766, 6, 437, 60, 1615, 19, 96, 27296, 60, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 29403, 71, 10, 3, 10354, 9, 7, 6, 11499, 6, 3519, 1496, 32, 17, 9, 6, 19343, 6, 6419, 1615, 19, 96, 1109, 1496, 32, 17, 9, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 29403, 71, 10, 3, 63, 14547, 6, 4301, 19828, 6, 6442, 9568, 6, 10123, 49, 6, 20404, 3432, 1615, 19, 96, 5715, 19828, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58]], 'inputs_pretokenized': ['Here\\'s a question and a few possible answers: \\n\\nQ: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nPossible A: park, coloring book, garden center, math problem, gravity\\n\\nWhy is \"math problem\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nPossible A: bus depot, beach, train station, bridge, bridge\\n\\nWhy is \"bridge\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A bad person places little value on being honest, acting without pretense or being what?\\nPossible A: excellent, upright, premium, competent, sincere\\n\\nWhy is \"sincere\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A bald eagle flies over St. Paul, where is it?\\nPossible A: texas, thermal, minnesota, canada, photograph\\n\\nWhy is \"minnesota\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nPossible A: yatch, corvette, aircraft carrier, destroyer, patrol boat\\n\\nWhy is \"corvette\" an answer aligned with human common sense? \\n'], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.']}\n",
      "{'inputs': [[947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 29403, 71, 10, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 6, 643, 13, 387, 6, 1679, 15, 26, 616, 6, 726, 2814, 7, 6, 3, 172, 32, 32, 1615, 19, 96, 2037, 15, 26, 616, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 29403, 71, 10, 281, 7092, 6, 2385, 8247, 6, 281, 1006, 6, 2007, 91, 6, 5727, 1615, 19, 96, 839, 7092, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 29403, 71, 10, 1590, 323, 6, 13418, 6, 577, 7860, 6, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 6, 582, 3165, 1615, 19, 96, 4895, 7860, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 29403, 71, 10, 3832, 3, 25911, 6, 2608, 6, 19270, 11686, 6, 248, 10962, 6, 7329, 4120, 1615, 19, 96, 20288, 10962, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 29403, 71, 10, 1886, 6, 7889, 6358, 6, 10412, 1530, 6, 1338, 6, 2078, 1615, 19, 96, 13442, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58]], 'inputs_pretokenized': ['Here\\'s a question and a few possible answers: \\n\\nQ: A beaver is know for building prowess, their supplies come from where?\\nPossible A: british columbia, body of water, wooded area, pay debts, zoo\\n\\nWhy is \"wooded area\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A car was hailed to chauffeur someone to the opera house, where was it heading?\\nPossible A: go downtown, appear suddenly, go fast, bottom out, east\\n\\nWhy is \"go downtown\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A child wants to play, what would they likely want?\\nPossible A: fall down, breathe, play tag, be dismembered by a chainsaw, become adult\\n\\nWhy is \"play tag\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A farmer sees a weasel in the woods, where is the farmer?\\nPossible A: chicken coop, beach, fairytale, great outdoors, corn fields\\n\\nWhy is \"great outdoors\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A gentleman is carrying equipment for golf, what is he likely to have?\\nPossible A: club, assembly hall, meditation center, meeting, church\\n\\nWhy is \"club\" an answer aligned with human common sense? \\n'], 'targets': [[2985, 35, 616, 19, 163, 8, 286, 21, 8, 36, 9, 624, 4471, 5, 1], [4375, 103, 762, 1], [7860, 315, 1155, 12, 577, 7860, 1], [1679, 7, 33, 398, 1], [3, 88, 2746, 3, 9, 1886, 1]], 'targets_pretokenized': ['\\nWooden area is only the place for the beaver supplies.', '\\nStudents do everything', '\\ntag different ways to play tag', '\\nwoods are must', '\\nhe wants a club']}\n",
      "First few examples from cos_e_v1.11_description_question_option_id:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 17011, 10, 71, 5, 2447, 272, 5, 7089, 484, 205, 5, 2004, 1530, 309, 5, 7270, 682, 262, 5, 18076], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 17011, 10, 71, 5, 2601, 14089, 272, 5, 2608, 205, 5, 2412, 2478, 309, 5, 4716, 262, 5, 4716], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 17011, 10, 71, 5, 1287, 272, 5, 23868, 205, 5, 3331, 309, 5, 11766, 262, 5, 437, 60], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 17011, 10, 71, 5, 3, 10354, 9, 7, 272, 5, 11499, 205, 5, 3519, 1496, 32, 17, 9, 309, 5, 19343, 262, 5, 6419], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 17011, 10, 71, 5, 3, 63, 14547, 272, 5, 4301, 19828, 205, 5, 6442, 9568, 309, 5, 10123, 49, 262, 5, 20404, 3432]], 'inputs_pretokenized': ['Pick the option in line with common sense to answer the question.\\nQuestion: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nOptions:\\n\\nA. park\\n\\nB. coloring book\\n\\nC. garden center\\n\\nD. math problem\\n\\nE. gravity\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nOptions:\\n\\nA. bus depot\\n\\nB. beach\\n\\nC. train station\\n\\nD. bridge\\n\\nE. bridge\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A bad person places little value on being honest, acting without pretense or being what?\\nOptions:\\n\\nA. excellent\\n\\nB. upright\\n\\nC. premium\\n\\nD. competent\\n\\nE. sincere\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A bald eagle flies over St. Paul, where is it?\\nOptions:\\n\\nA. texas\\n\\nB. thermal\\n\\nC. minnesota\\n\\nD. canada\\n\\nE. photograph\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nOptions:\\n\\nA. yatch\\n\\nB. corvette\\n\\nC. aircraft carrier\\n\\nD. destroyer\\n\\nE. patrol boat\\n\\n'], 'targets': [[309, 1], [309, 1], [262, 1], [205, 1], [272, 1]], 'targets_pretokenized': ['\\nD', '\\nD', '\\nE', '\\nC', '\\nB']}\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 17011, 10, 71, 5, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 272, 5, 643, 13, 387, 205, 5, 1679, 15, 26, 616, 309, 5, 726, 2814, 7, 262, 5, 3, 172, 32, 32], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 17011, 10, 71, 5, 281, 7092, 272, 5, 2385, 8247, 205, 5, 281, 1006, 309, 5, 2007, 91, 262, 5, 5727], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 17011, 10, 71, 5, 1590, 323, 272, 5, 13418, 205, 5, 577, 7860, 309, 5, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 262, 5, 582, 3165], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 17011, 10, 71, 5, 3832, 3, 25911, 272, 5, 2608, 205, 5, 19270, 11686, 309, 5, 248, 10962, 262, 5, 7329, 4120], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 11860, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 17011, 10, 71, 5, 1886, 272, 5, 7889, 6358, 205, 5, 10412, 1530, 309, 5, 1338, 262, 5, 2078]], 'inputs_pretokenized': ['Pick the option in line with common sense to answer the question.\\nQuestion: A beaver is know for building prowess, their supplies come from where?\\nOptions:\\n\\nA. british columbia\\n\\nB. body of water\\n\\nC. wooded area\\n\\nD. pay debts\\n\\nE. zoo\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A car was hailed to chauffeur someone to the opera house, where was it heading?\\nOptions:\\n\\nA. go downtown\\n\\nB. appear suddenly\\n\\nC. go fast\\n\\nD. bottom out\\n\\nE. east\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A child wants to play, what would they likely want?\\nOptions:\\n\\nA. fall down\\n\\nB. breathe\\n\\nC. play tag\\n\\nD. be dismembered by a chainsaw\\n\\nE. become adult\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A farmer sees a weasel in the woods, where is the farmer?\\nOptions:\\n\\nA. chicken coop\\n\\nB. beach\\n\\nC. fairytale\\n\\nD. great outdoors\\n\\nE. corn fields\\n\\n', 'Pick the option in line with common sense to answer the question.\\nQuestion: A gentleman is carrying equipment for golf, what is he likely to have?\\nOptions:\\n\\nA. club\\n\\nB. assembly hall\\n\\nC. meditation center\\n\\nD. meeting\\n\\nE. church\\n\\n'], 'targets': [[205, 1], [71, 1], [205, 1], [309, 1], [71, 1]], 'targets_pretokenized': ['\\nC', '\\nA', '\\nC', '\\nD', '\\nA']}\n",
      "First few examples from cos_e_v1.11_description_question_option_text:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['park', 'coloring book', 'garden center', 'math problem', 'gravity'], ['bus depot', 'beach', 'train station', 'bridge', 'bridge'], ['excellent', 'upright', 'premium', 'competent', 'sincere'], ['texas', 'thermal', 'minnesota', 'canada', 'photograph'], ['yatch', 'corvette', 'aircraft carrier', 'destroyer', 'patrol boat']], 'inputs': [[8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 17011, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 17011, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 17011, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 17011, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 17011, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432]], 'inputs_pretokenized': ['Pick the option in line with common sense to answer the question.\\nQuestions: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nOptions:\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nOptions:\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A bad person places little value on being honest, acting without pretense or being what?\\nOptions:\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A bald eagle flies over St. Paul, where is it?\\nOptions:\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nOptions:\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n'], 'targets': [[7270, 682, 1], [4716, 1], [437, 60, 1], [3519, 1496, 32, 17, 9, 1], [4301, 19828, 1]], 'targets_pretokenized': ['\\nmath problem', '\\nbridge', '\\nsincere', '\\nminnesota', '\\ncorvette']}\n",
      "{'answer_choices': [['british columbia', 'body of water', 'wooded area', 'pay debts', 'zoo'], ['go downtown', 'appear suddenly', 'go fast', 'bottom out', 'east'], ['fall down', 'breathe', 'play tag', 'be dismembered by a chainsaw', 'become adult'], ['chicken coop', 'beach', 'fairytale', 'great outdoors', 'corn fields'], ['club', 'assembly hall', 'meditation center', 'meeting', 'church']], 'inputs': [[8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 17011, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 17011, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 17011, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 17011, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120], [8356, 8, 1182, 16, 689, 28, 1017, 1254, 12, 1525, 8, 822, 5, 14218, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 17011, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078]], 'inputs_pretokenized': ['Pick the option in line with common sense to answer the question.\\nQuestions: A beaver is know for building prowess, their supplies come from where?\\nOptions:\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A car was hailed to chauffeur someone to the opera house, where was it heading?\\nOptions:\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A child wants to play, what would they likely want?\\nOptions:\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A farmer sees a weasel in the woods, where is the farmer?\\nOptions:\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n', 'Pick the option in line with common sense to answer the question.\\nQuestions: A gentleman is carrying equipment for golf, what is he likely to have?\\nOptions:\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n'], 'targets': [[1679, 15, 26, 616, 1], [281, 7092, 1], [577, 7860, 1], [248, 10962, 1], [1886, 1]], 'targets_pretokenized': ['\\nwooded area', '\\ngo downtown', '\\nplay tag', '\\ngreat outdoors', '\\nclub']}\n",
      "First few examples from cos_e_v1.11_explain_why_human:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'inputs': [[11860, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 17011, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076, 25488, 572, 3, 9, 936, 133, 854, 96, 3357, 107, 682, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 17011, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716, 25488, 572, 3, 9, 936, 133, 854, 96, 9818, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 17011, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60, 25488, 572, 3, 9, 936, 133, 854, 96, 27296, 60, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 17011, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419, 25488, 572, 3, 9, 936, 133, 854, 96, 1109, 1496, 32, 17, 9, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 17011, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432, 25488, 572, 3, 9, 936, 133, 854, 96, 5715, 19828, 121, 12, 1525, 8, 822, 756, 10]], 'inputs_pretokenized': ['Question: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nOptions:\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n\\nExplain why a human would choose \"math problem\" to answer the question above:\\n', 'Question: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nOptions:\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n\\nExplain why a human would choose \"bridge\" to answer the question above:\\n', 'Question: A bad person places little value on being honest, acting without pretense or being what?\\nOptions:\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n\\nExplain why a human would choose \"sincere\" to answer the question above:\\n', 'Question: A bald eagle flies over St. Paul, where is it?\\nOptions:\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n\\nExplain why a human would choose \"minnesota\" to answer the question above:\\n', 'Question: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nOptions:\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n\\nExplain why a human would choose \"corvette\" to answer the question above:\\n'], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.']}\n",
      "{'inputs': [[11860, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 17011, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32, 25488, 572, 3, 9, 936, 133, 854, 96, 2037, 15, 26, 616, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 17011, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727, 25488, 572, 3, 9, 936, 133, 854, 96, 839, 7092, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 17011, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165, 25488, 572, 3, 9, 936, 133, 854, 96, 4895, 7860, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 17011, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120, 25488, 572, 3, 9, 936, 133, 854, 96, 20288, 10962, 121, 12, 1525, 8, 822, 756, 10], [11860, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 17011, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078, 25488, 572, 3, 9, 936, 133, 854, 96, 13442, 121, 12, 1525, 8, 822, 756, 10]], 'inputs_pretokenized': ['Question: A beaver is know for building prowess, their supplies come from where?\\nOptions:\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n\\nExplain why a human would choose \"wooded area\" to answer the question above:\\n', 'Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\\nOptions:\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n\\nExplain why a human would choose \"go downtown\" to answer the question above:\\n', 'Question: A child wants to play, what would they likely want?\\nOptions:\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n\\nExplain why a human would choose \"play tag\" to answer the question above:\\n', 'Question: A farmer sees a weasel in the woods, where is the farmer?\\nOptions:\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n\\nExplain why a human would choose \"great outdoors\" to answer the question above:\\n', 'Question: A gentleman is carrying equipment for golf, what is he likely to have?\\nOptions:\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n\\nExplain why a human would choose \"club\" to answer the question above:\\n'], 'targets': [[2985, 35, 616, 19, 163, 8, 286, 21, 8, 36, 9, 624, 4471, 5, 1], [4375, 103, 762, 1], [7860, 315, 1155, 12, 577, 7860, 1], [1679, 7, 33, 398, 1], [3, 88, 2746, 3, 9, 1886, 1]], 'targets_pretokenized': ['\\nWooden area is only the place for the beaver supplies.', '\\nStudents do everything', '\\ntag different ways to play tag', '\\nwoods are must', '\\nhe wants a club']}\n",
      "First few examples from cos_e_v1.11_generate_explanation_given_text:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'inputs': [[11860, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 17011, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076, 37, 1525, 19, 96, 3357, 107, 682, 121, 250], [11860, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 17011, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716, 37, 1525, 19, 96, 9818, 121, 250], [11860, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 17011, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60, 37, 1525, 19, 96, 27296, 60, 121, 250], [11860, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 17011, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419, 37, 1525, 19, 96, 1109, 1496, 32, 17, 9, 121, 250], [11860, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 17011, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432, 37, 1525, 19, 96, 5715, 19828, 121, 250]], 'inputs_pretokenized': ['Question: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nOptions:\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n\\nThe answer is \"math problem\" because\\n', 'Question: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nOptions:\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n\\nThe answer is \"bridge\" because\\n', 'Question: A bad person places little value on being honest, acting without pretense or being what?\\nOptions:\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n\\nThe answer is \"sincere\" because\\n', 'Question: A bald eagle flies over St. Paul, where is it?\\nOptions:\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n\\nThe answer is \"minnesota\" because\\n', 'Question: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nOptions:\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n\\nThe answer is \"corvette\" because\\n'], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.']}\n",
      "{'inputs': [[11860, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 17011, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32, 37, 1525, 19, 96, 2037, 15, 26, 616, 121, 250], [11860, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 17011, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727, 37, 1525, 19, 96, 839, 7092, 121, 250], [11860, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 17011, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165, 37, 1525, 19, 96, 4895, 7860, 121, 250], [11860, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 17011, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120, 37, 1525, 19, 96, 20288, 10962, 121, 250], [11860, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 17011, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078, 37, 1525, 19, 96, 13442, 121, 250]], 'inputs_pretokenized': ['Question: A beaver is know for building prowess, their supplies come from where?\\nOptions:\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n\\nThe answer is \"wooded area\" because\\n', 'Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\\nOptions:\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n\\nThe answer is \"go downtown\" because\\n', 'Question: A child wants to play, what would they likely want?\\nOptions:\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n\\nThe answer is \"play tag\" because\\n', 'Question: A farmer sees a weasel in the woods, where is the farmer?\\nOptions:\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n\\nThe answer is \"great outdoors\" because\\n', 'Question: A gentleman is carrying equipment for golf, what is he likely to have?\\nOptions:\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n\\nThe answer is \"club\" because\\n'], 'targets': [[2985, 35, 616, 19, 163, 8, 286, 21, 8, 36, 9, 624, 4471, 5, 1], [4375, 103, 762, 1], [7860, 315, 1155, 12, 577, 7860, 1], [1679, 7, 33, 398, 1], [3, 88, 2746, 3, 9, 1886, 1]], 'targets_pretokenized': ['\\nWooden area is only the place for the beaver supplies.', '\\nStudents do everything', '\\ntag different ways to play tag', '\\nwoods are must', '\\nhe wants a club']}\n",
      "First few examples from cos_e_v1.11_i_think:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'inputs': [[947, 31, 7, 3, 9, 822, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076, 27, 857, 8, 2024, 1160, 19, 96, 3357, 107, 682, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716, 27, 857, 8, 2024, 1160, 19, 96, 9818, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60, 27, 857, 8, 2024, 1160, 19, 96, 27296, 60, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419, 27, 857, 8, 2024, 1160, 19, 96, 1109, 1496, 32, 17, 9, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432, 27, 857, 8, 2024, 1160, 19, 96, 5715, 19828, 1686, 270, 31, 7, 572, 10]], 'inputs_pretokenized': ['Here\\'s a question: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\n\\nHere are possible answers to this question:\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n\\nI believe the correct choice is \"math problem\", here\\'s why:\\n', 'Here\\'s a question: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\n\\nHere are possible answers to this question:\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n\\nI believe the correct choice is \"bridge\", here\\'s why:\\n', 'Here\\'s a question: A bad person places little value on being honest, acting without pretense or being what?\\n\\nHere are possible answers to this question:\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n\\nI believe the correct choice is \"sincere\", here\\'s why:\\n', 'Here\\'s a question: A bald eagle flies over St. Paul, where is it?\\n\\nHere are possible answers to this question:\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n\\nI believe the correct choice is \"minnesota\", here\\'s why:\\n', 'Here\\'s a question: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\n\\nHere are possible answers to this question:\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n\\nI believe the correct choice is \"corvette\", here\\'s why:\\n'], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.']}\n",
      "{'inputs': [[947, 31, 7, 3, 9, 822, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32, 27, 857, 8, 2024, 1160, 19, 96, 2037, 15, 26, 616, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727, 27, 857, 8, 2024, 1160, 19, 96, 839, 7092, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165, 27, 857, 8, 2024, 1160, 19, 96, 4895, 7860, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120, 27, 857, 8, 2024, 1160, 19, 96, 20288, 10962, 1686, 270, 31, 7, 572, 10], [947, 31, 7, 3, 9, 822, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 947, 33, 487, 4269, 12, 48, 822, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078, 27, 857, 8, 2024, 1160, 19, 96, 13442, 1686, 270, 31, 7, 572, 10]], 'inputs_pretokenized': ['Here\\'s a question: A beaver is know for building prowess, their supplies come from where?\\n\\nHere are possible answers to this question:\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n\\nI believe the correct choice is \"wooded area\", here\\'s why:\\n', 'Here\\'s a question: A car was hailed to chauffeur someone to the opera house, where was it heading?\\n\\nHere are possible answers to this question:\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n\\nI believe the correct choice is \"go downtown\", here\\'s why:\\n', 'Here\\'s a question: A child wants to play, what would they likely want?\\n\\nHere are possible answers to this question:\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n\\nI believe the correct choice is \"play tag\", here\\'s why:\\n', 'Here\\'s a question: A farmer sees a weasel in the woods, where is the farmer?\\n\\nHere are possible answers to this question:\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n\\nI believe the correct choice is \"great outdoors\", here\\'s why:\\n', 'Here\\'s a question: A gentleman is carrying equipment for golf, what is he likely to have?\\n\\nHere are possible answers to this question:\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n\\nI believe the correct choice is \"club\", here\\'s why:\\n'], 'targets': [[2985, 35, 616, 19, 163, 8, 286, 21, 8, 36, 9, 624, 4471, 5, 1], [4375, 103, 762, 1], [7860, 315, 1155, 12, 577, 7860, 1], [1679, 7, 33, 398, 1], [3, 88, 2746, 3, 9, 1886, 1]], 'targets_pretokenized': ['\\nWooden area is only the place for the beaver supplies.', '\\nStudents do everything', '\\ntag different ways to play tag', '\\nwoods are must', '\\nhe wants a club']}\n",
      "First few examples from cos_e_v1.11_question_description_option_id:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 2447, 272, 5, 7089, 484, 205, 5, 2004, 1530, 309, 5, 7270, 682, 262, 5, 18076], [71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 2601, 14089, 272, 5, 2608, 205, 5, 2412, 2478, 309, 5, 4716, 262, 5, 4716], [71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 1287, 272, 5, 23868, 205, 5, 3331, 309, 5, 11766, 262, 5, 437, 60], [71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 3, 10354, 9, 7, 272, 5, 11499, 205, 5, 3519, 1496, 32, 17, 9, 309, 5, 19343, 262, 5, 6419], [71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 3, 63, 14547, 272, 5, 4301, 19828, 205, 5, 6442, 9568, 309, 5, 10123, 49, 262, 5, 20404, 3432]], 'inputs_pretokenized': ['\"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. park\\n\\nB. coloring book\\n\\nC. garden center\\n\\nD. math problem\\n\\nE. gravity\\n\\n', 'A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. bus depot\\n\\nB. beach\\n\\nC. train station\\n\\nD. bridge\\n\\nE. bridge\\n\\n', 'A bad person places little value on being honest, acting without pretense or being what?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. excellent\\n\\nB. upright\\n\\nC. premium\\n\\nD. competent\\n\\nE. sincere\\n\\n', 'A bald eagle flies over St. Paul, where is it?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. texas\\n\\nB. thermal\\n\\nC. minnesota\\n\\nD. canada\\n\\nE. photograph\\n\\n', 'A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. yatch\\n\\nB. corvette\\n\\nC. aircraft carrier\\n\\nD. destroyer\\n\\nE. patrol boat\\n\\n'], 'targets': [[309, 1], [309, 1], [262, 1], [205, 1], [272, 1]], 'targets_pretokenized': ['\\nD', '\\nD', '\\nE', '\\nC', '\\nB']}\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 272, 5, 643, 13, 387, 205, 5, 1679, 15, 26, 616, 309, 5, 726, 2814, 7, 262, 5, 3, 172, 32, 32], [71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 281, 7092, 272, 5, 2385, 8247, 205, 5, 281, 1006, 309, 5, 2007, 91, 262, 5, 5727], [71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 1590, 323, 272, 5, 13418, 205, 5, 577, 7860, 309, 5, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 262, 5, 582, 3165], [71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 3832, 3, 25911, 272, 5, 2608, 205, 5, 19270, 11686, 309, 5, 248, 10962, 262, 5, 7329, 4120], [71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 71, 5, 1886, 272, 5, 7889, 6358, 205, 5, 10412, 1530, 309, 5, 1338, 262, 5, 2078]], 'inputs_pretokenized': ['A beaver is know for building prowess, their supplies come from where?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. british columbia\\n\\nB. body of water\\n\\nC. wooded area\\n\\nD. pay debts\\n\\nE. zoo\\n\\n', 'A car was hailed to chauffeur someone to the opera house, where was it heading?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. go downtown\\n\\nB. appear suddenly\\n\\nC. go fast\\n\\nD. bottom out\\n\\nE. east\\n\\n', 'A child wants to play, what would they likely want?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. fall down\\n\\nB. breathe\\n\\nC. play tag\\n\\nD. be dismembered by a chainsaw\\n\\nE. become adult\\n\\n', 'A farmer sees a weasel in the woods, where is the farmer?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. chicken coop\\n\\nB. beach\\n\\nC. fairytale\\n\\nD. great outdoors\\n\\nE. corn fields\\n\\n', 'A gentleman is carrying equipment for golf, what is he likely to have?\\nChoose the most suitable option to answer the above question.\\nOptions：\\n\\nA. club\\n\\nB. assembly hall\\n\\nC. meditation center\\n\\nD. meeting\\n\\nE. church\\n\\n'], 'targets': [[205, 1], [71, 1], [205, 1], [309, 1], [71, 1]], 'targets_pretokenized': ['\\nC', '\\nA', '\\nC', '\\nD', '\\nA']}\n",
      "First few examples from cos_e_v1.11_question_description_option_text:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['park', 'coloring book', 'garden center', 'math problem', 'gravity'], ['bus depot', 'beach', 'train station', 'bridge', 'bridge'], ['excellent', 'upright', 'premium', 'competent', 'sincere'], ['texas', 'thermal', 'minnesota', 'canada', 'photograph'], ['yatch', 'corvette', 'aircraft carrier', 'destroyer', 'patrol boat']], 'inputs': [[96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076], [71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716], [71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60], [71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419], [71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432]], 'inputs_pretokenized': ['\"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n', 'A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n', 'A bad person places little value on being honest, acting without pretense or being what?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n', 'A bald eagle flies over St. Paul, where is it?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n', 'A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n'], 'targets': [[7270, 682, 1], [4716, 1], [437, 60, 1], [3519, 1496, 32, 17, 9, 1], [4301, 19828, 1]], 'targets_pretokenized': ['\\nmath problem', '\\nbridge', '\\nsincere', '\\nminnesota', '\\ncorvette']}\n",
      "{'answer_choices': [['british columbia', 'body of water', 'wooded area', 'pay debts', 'zoo'], ['go downtown', 'appear suddenly', 'go fast', 'bottom out', 'east'], ['fall down', 'breathe', 'play tag', 'be dismembered by a chainsaw', 'become adult'], ['chicken coop', 'beach', 'fairytale', 'great outdoors', 'corn fields'], ['club', 'assembly hall', 'meditation center', 'meeting', 'church']], 'inputs': [[71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32], [71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727], [71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165], [71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120], [71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 7023, 8, 167, 3255, 1182, 12, 1525, 8, 756, 822, 5, 17011, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078]], 'inputs_pretokenized': ['A beaver is know for building prowess, their supplies come from where?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n', 'A car was hailed to chauffeur someone to the opera house, where was it heading?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n', 'A child wants to play, what would they likely want?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n', 'A farmer sees a weasel in the woods, where is the farmer?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n', 'A gentleman is carrying equipment for golf, what is he likely to have?\\nChoose the most suitable option to answer the above question.\\nOptions:\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n'], 'targets': [[1679, 15, 26, 616, 1], [281, 7092, 1], [577, 7860, 1], [248, 10962, 1], [1886, 1]], 'targets_pretokenized': ['\\nwooded area', '\\ngo downtown', '\\nplay tag', '\\ngreat outdoors', '\\nclub']}\n",
      "First few examples from cos_e_v1.11_question_option_description_id:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 71, 5, 2447, 272, 5, 7089, 484, 205, 5, 2004, 1530, 309, 5, 7270, 682, 262, 5, 18076, 37, 200, 1525, 19], [71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 71, 5, 2601, 14089, 272, 5, 2608, 205, 5, 2412, 2478, 309, 5, 4716, 262, 5, 4716, 37, 200, 1525, 19], [71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 71, 5, 1287, 272, 5, 23868, 205, 5, 3331, 309, 5, 11766, 262, 5, 437, 60, 37, 200, 1525, 19], [71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 71, 5, 3, 10354, 9, 7, 272, 5, 11499, 205, 5, 3519, 1496, 32, 17, 9, 309, 5, 19343, 262, 5, 6419, 37, 200, 1525, 19], [71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 71, 5, 3, 63, 14547, 272, 5, 4301, 19828, 205, 5, 6442, 9568, 309, 5, 10123, 49, 262, 5, 20404, 3432, 37, 200, 1525, 19]], 'inputs_pretokenized': ['\"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\n\\nA. park\\n\\nB. coloring book\\n\\nC. garden center\\n\\nD. math problem\\n\\nE. gravity\\n\\nThe best answer is\\n', 'A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\n\\nA. bus depot\\n\\nB. beach\\n\\nC. train station\\n\\nD. bridge\\n\\nE. bridge\\n\\nThe best answer is\\n', 'A bad person places little value on being honest, acting without pretense or being what?\\n\\nA. excellent\\n\\nB. upright\\n\\nC. premium\\n\\nD. competent\\n\\nE. sincere\\n\\nThe best answer is\\n', 'A bald eagle flies over St. Paul, where is it?\\n\\nA. texas\\n\\nB. thermal\\n\\nC. minnesota\\n\\nD. canada\\n\\nE. photograph\\n\\nThe best answer is\\n', 'A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\n\\nA. yatch\\n\\nB. corvette\\n\\nC. aircraft carrier\\n\\nD. destroyer\\n\\nE. patrol boat\\n\\nThe best answer is\\n'], 'targets': [[309, 1], [309, 1], [262, 1], [205, 1], [272, 1]], 'targets_pretokenized': ['\\nD', '\\nD', '\\nE', '\\nC', '\\nB']}\n",
      "{'answer_choices': [['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E'], ['A', 'B', 'C', 'D', 'E']], 'inputs': [[71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 71, 5, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 272, 5, 643, 13, 387, 205, 5, 1679, 15, 26, 616, 309, 5, 726, 2814, 7, 262, 5, 3, 172, 32, 32, 37, 200, 1525, 19], [71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 71, 5, 281, 7092, 272, 5, 2385, 8247, 205, 5, 281, 1006, 309, 5, 2007, 91, 262, 5, 5727, 37, 200, 1525, 19], [71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 71, 5, 1590, 323, 272, 5, 13418, 205, 5, 577, 7860, 309, 5, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 262, 5, 582, 3165, 37, 200, 1525, 19], [71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 71, 5, 3832, 3, 25911, 272, 5, 2608, 205, 5, 19270, 11686, 309, 5, 248, 10962, 262, 5, 7329, 4120, 37, 200, 1525, 19], [71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 71, 5, 1886, 272, 5, 7889, 6358, 205, 5, 10412, 1530, 309, 5, 1338, 262, 5, 2078, 37, 200, 1525, 19]], 'inputs_pretokenized': ['A beaver is know for building prowess, their supplies come from where?\\n\\nA. british columbia\\n\\nB. body of water\\n\\nC. wooded area\\n\\nD. pay debts\\n\\nE. zoo\\n\\nThe best answer is\\n', 'A car was hailed to chauffeur someone to the opera house, where was it heading?\\n\\nA. go downtown\\n\\nB. appear suddenly\\n\\nC. go fast\\n\\nD. bottom out\\n\\nE. east\\n\\nThe best answer is\\n', 'A child wants to play, what would they likely want?\\n\\nA. fall down\\n\\nB. breathe\\n\\nC. play tag\\n\\nD. be dismembered by a chainsaw\\n\\nE. become adult\\n\\nThe best answer is\\n', 'A farmer sees a weasel in the woods, where is the farmer?\\n\\nA. chicken coop\\n\\nB. beach\\n\\nC. fairytale\\n\\nD. great outdoors\\n\\nE. corn fields\\n\\nThe best answer is\\n', 'A gentleman is carrying equipment for golf, what is he likely to have?\\n\\nA. club\\n\\nB. assembly hall\\n\\nC. meditation center\\n\\nD. meeting\\n\\nE. church\\n\\nThe best answer is\\n'], 'targets': [[205, 1], [71, 1], [205, 1], [309, 1], [71, 1]], 'targets_pretokenized': ['\\nC', '\\nA', '\\nC', '\\nD', '\\nA']}\n",
      "First few examples from cos_e_v1.11_question_option_description_text:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'answer_choices': [['park', 'coloring book', 'garden center', 'math problem', 'gravity'], ['bus depot', 'beach', 'train station', 'bridge', 'bridge'], ['excellent', 'upright', 'premium', 'competent', 'sincere'], ['texas', 'thermal', 'minnesota', 'canada', 'photograph'], ['yatch', 'corvette', 'aircraft carrier', 'destroyer', 'patrol boat']], 'inputs': [[96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076, 37, 200, 1525, 19], [71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716, 37, 200, 1525, 19], [71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60, 37, 200, 1525, 19], [71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419, 37, 200, 1525, 19], [71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432, 37, 200, 1525, 19]], 'inputs_pretokenized': ['\"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n\\nThe best answer is\\n', 'A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n\\nThe best answer is\\n', 'A bad person places little value on being honest, acting without pretense or being what?\\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n\\nThe best answer is\\n', 'A bald eagle flies over St. Paul, where is it?\\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n\\nThe best answer is\\n', 'A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n\\nThe best answer is\\n'], 'targets': [[7270, 682, 1], [4716, 1], [437, 60, 1], [3519, 1496, 32, 17, 9, 1], [4301, 19828, 1]], 'targets_pretokenized': ['\\nmath problem', '\\nbridge', '\\nsincere', '\\nminnesota', '\\ncorvette']}\n",
      "{'answer_choices': [['british columbia', 'body of water', 'wooded area', 'pay debts', 'zoo'], ['go downtown', 'appear suddenly', 'go fast', 'bottom out', 'east'], ['fall down', 'breathe', 'play tag', 'be dismembered by a chainsaw', 'become adult'], ['chicken coop', 'beach', 'fairytale', 'great outdoors', 'corn fields'], ['club', 'assembly hall', 'meditation center', 'meeting', 'church']], 'inputs': [[71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32, 37, 200, 1525, 19], [71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727, 37, 200, 1525, 19], [71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165, 37, 200, 1525, 19], [71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120, 37, 200, 1525, 19], [71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078, 37, 200, 1525, 19]], 'inputs_pretokenized': ['A beaver is know for building prowess, their supplies come from where?\\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n\\nThe best answer is\\n', 'A car was hailed to chauffeur someone to the opera house, where was it heading?\\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n\\nThe best answer is\\n', 'A child wants to play, what would they likely want?\\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n\\nThe best answer is\\n', 'A farmer sees a weasel in the woods, where is the farmer?\\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n\\nThe best answer is\\n', 'A gentleman is carrying equipment for golf, what is he likely to have?\\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n\\nThe best answer is\\n'], 'targets': [[1679, 15, 26, 616, 1], [281, 7092, 1], [577, 7860, 1], [248, 10962, 1], [1886, 1]], 'targets_pretokenized': ['\\nwooded area', '\\ngo downtown', '\\nplay tag', '\\ngreat outdoors', '\\nclub']}\n",
      "First few examples from cos_e_v1.11_rationale:\n",
      "dict_keys(['train', 'validation'])\n",
      "{'inputs': [[11860, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 13745, 7, 10, 3, 18, 2447, 3, 18, 7089, 484, 3, 18, 2004, 1530, 3, 18, 7270, 682, 3, 18, 18076, 37, 12226, 15, 12, 854, 96, 3357, 107, 682, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 13745, 7, 10, 3, 18, 2601, 14089, 3, 18, 2608, 3, 18, 2412, 2478, 3, 18, 4716, 3, 18, 4716, 37, 12226, 15, 12, 854, 96, 9818, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 13745, 7, 10, 3, 18, 1287, 3, 18, 23868, 3, 18, 3331, 3, 18, 11766, 3, 18, 437, 60, 37, 12226, 15, 12, 854, 96, 27296, 60, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 13745, 7, 10, 3, 18, 3, 10354, 9, 7, 3, 18, 11499, 3, 18, 3519, 1496, 32, 17, 9, 3, 18, 19343, 3, 18, 6419, 37, 12226, 15, 12, 854, 96, 1109, 1496, 32, 17, 9, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 13745, 7, 10, 3, 18, 3, 63, 14547, 3, 18, 4301, 19828, 3, 18, 6442, 9568, 3, 18, 10123, 49, 3, 18, 20404, 3432, 37, 12226, 15, 12, 854, 96, 5715, 19828, 121, 38, 8, 1525, 19, 24, 10]], 'inputs_pretokenized': ['Question: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\n\\nChoices: \\n- park\\n- coloring book\\n- garden center\\n- math problem\\n- gravity\\n\\nThe rationale to choose \"math problem\" as the answer is that: ', 'Question: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\n\\nChoices: \\n- bus depot\\n- beach\\n- train station\\n- bridge\\n- bridge\\n\\nThe rationale to choose \"bridge\" as the answer is that: ', 'Question: A bad person places little value on being honest, acting without pretense or being what?\\n\\nChoices: \\n- excellent\\n- upright\\n- premium\\n- competent\\n- sincere\\n\\nThe rationale to choose \"sincere\" as the answer is that: ', 'Question: A bald eagle flies over St. Paul, where is it?\\n\\nChoices: \\n- texas\\n- thermal\\n- minnesota\\n- canada\\n- photograph\\n\\nThe rationale to choose \"minnesota\" as the answer is that: ', 'Question: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\n\\nChoices: \\n- yatch\\n- corvette\\n- aircraft carrier\\n- destroyer\\n- patrol boat\\n\\nThe rationale to choose \"corvette\" as the answer is that: '], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.']}\n",
      "{'inputs': [[11860, 10, 71, 36, 9, 624, 19, 214, 21, 740, 813, 1123, 7, 7, 6, 70, 4471, 369, 45, 213, 58, 13745, 7, 10, 3, 18, 3, 2160, 17, 1273, 7632, 5937, 23, 9, 3, 18, 643, 13, 387, 3, 18, 1679, 15, 26, 616, 3, 18, 726, 2814, 7, 3, 18, 3, 172, 32, 32, 37, 12226, 15, 12, 854, 96, 2037, 15, 26, 616, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 443, 47, 3, 107, 10990, 12, 28892, 841, 12, 8, 6329, 629, 6, 213, 47, 34, 6904, 58, 13745, 7, 10, 3, 18, 281, 7092, 3, 18, 2385, 8247, 3, 18, 281, 1006, 3, 18, 2007, 91, 3, 18, 5727, 37, 12226, 15, 12, 854, 96, 839, 7092, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 861, 2746, 12, 577, 6, 125, 133, 79, 952, 241, 58, 13745, 7, 10, 3, 18, 1590, 323, 3, 18, 13418, 3, 18, 577, 7860, 3, 18, 36, 1028, 12066, 15, 26, 57, 3, 9, 3741, 13125, 3, 18, 582, 3165, 37, 12226, 15, 12, 854, 96, 4895, 7860, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 17766, 217, 7, 3, 9, 62, 9, 7, 15, 40, 16, 8, 1679, 7, 6, 213, 19, 8, 17766, 58, 13745, 7, 10, 3, 18, 3832, 3, 25911, 3, 18, 2608, 3, 18, 19270, 11686, 3, 18, 248, 10962, 3, 18, 7329, 4120, 37, 12226, 15, 12, 854, 96, 20288, 10962, 121, 38, 8, 1525, 19, 24, 10], [11860, 10, 71, 26266, 19, 6771, 1277, 21, 4319, 6, 125, 19, 3, 88, 952, 12, 43, 58, 13745, 7, 10, 3, 18, 1886, 3, 18, 7889, 6358, 3, 18, 10412, 1530, 3, 18, 1338, 3, 18, 2078, 37, 12226, 15, 12, 854, 96, 13442, 121, 38, 8, 1525, 19, 24, 10]], 'inputs_pretokenized': ['Question: A beaver is know for building prowess, their supplies come from where?\\n\\nChoices: \\n- british columbia\\n- body of water\\n- wooded area\\n- pay debts\\n- zoo\\n\\nThe rationale to choose \"wooded area\" as the answer is that: ', 'Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\\n\\nChoices: \\n- go downtown\\n- appear suddenly\\n- go fast\\n- bottom out\\n- east\\n\\nThe rationale to choose \"go downtown\" as the answer is that: ', 'Question: A child wants to play, what would they likely want?\\n\\nChoices: \\n- fall down\\n- breathe\\n- play tag\\n- be dismembered by a chainsaw\\n- become adult\\n\\nThe rationale to choose \"play tag\" as the answer is that: ', 'Question: A farmer sees a weasel in the woods, where is the farmer?\\n\\nChoices: \\n- chicken coop\\n- beach\\n- fairytale\\n- great outdoors\\n- corn fields\\n\\nThe rationale to choose \"great outdoors\" as the answer is that: ', 'Question: A gentleman is carrying equipment for golf, what is he likely to have?\\n\\nChoices: \\n- club\\n- assembly hall\\n- meditation center\\n- meeting\\n- church\\n\\nThe rationale to choose \"club\" as the answer is that: '], 'targets': [[2985, 35, 616, 19, 163, 8, 286, 21, 8, 36, 9, 624, 4471, 5, 1], [4375, 103, 762, 1], [7860, 315, 1155, 12, 577, 7860, 1], [1679, 7, 33, 398, 1], [3, 88, 2746, 3, 9, 1886, 1]], 'targets_pretokenized': ['\\nWooden area is only the place for the beaver supplies.', '\\nStudents do everything', '\\ntag different ways to play tag', '\\nwoods are must', '\\nhe wants a club']}\n"
     ]
    }
   ],
   "source": [
    "# Load and print the first few examples from each split\n",
    "split_data = {}\n",
    "for split in splits:\n",
    "    split_data[split] = load_dataset(\"bigscience/P3\", split)\n",
    "    print(f\"First few examples from {split}:\")\n",
    "    print(split_data[split].keys())\n",
    "    print(split_data[split]['train'][:5])\n",
    "    print(split_data[split]['validation'][:5])\n",
    "\n",
    "data = split_data['cos_e_v1.11_aligned_with_common_sense']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 9535 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     logging_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./logs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39mpeft_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1838\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1839\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1840\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    452\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2799\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, keys: List) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m   2798\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2799\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(keys)\n\u001b[1;32m   2800\u001b[0m     n_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch))])\n\u001b[1;32m   2801\u001b[0m     \u001b[39mreturn\u001b[39;00m [{col: array[i] \u001b[39mfor\u001b[39;00m col, array \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2779\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2778\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2779\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2780\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2781\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2782\u001b[0m )\n\u001b[1;32m   2783\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:583\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 583\u001b[0m     _check_valid_index_key(key, size)\n\u001b[1;32m    584\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:536\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Iterable):\n\u001b[1;32m    535\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(key) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 536\u001b[0m         _check_valid_index_key(\u001b[39mint\u001b[39;49m(\u001b[39mmax\u001b[39;49m(key)), size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    537\u001b[0m         _check_valid_index_key(\u001b[39mint\u001b[39m(\u001b[39mmin\u001b[39m(key)), size\u001b[39m=\u001b[39msize)\n\u001b[1;32m    538\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:526\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m (key \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m key \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[0;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 9535 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['validation'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
