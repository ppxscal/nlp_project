{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': [[947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 96, 7238, 33, 335, 16981, 30, 46, 8947, 2195, 5, 5245, 1590, 326, 5, 852, 132, 33, 3, 4, 16981, 535, 363, 19, 48, 46, 677, 13, 58, 29403, 71, 10, 2447, 6, 7089, 484, 6, 2004, 1530, 6, 7270, 682, 6, 18076, 1615, 19, 96, 3357, 107, 682, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 1079, 19, 3, 9, 8524, 51, 5, 8718, 114, 8, 26524, 6, 3, 88, 1342, 1084, 48, 1843, 13, 5127, 3620, 5, 2840, 405, 3, 88, 619, 58, 29403, 71, 10, 2601, 14089, 6, 2608, 6, 2412, 2478, 6, 4716, 6, 4716, 1615, 19, 96, 9818, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 1282, 568, 1747, 385, 701, 30, 271, 5057, 6, 6922, 406, 7140, 5167, 42, 271, 125, 58, 29403, 71, 10, 1287, 6, 23868, 6, 3331, 6, 11766, 6, 437, 60, 1615, 19, 96, 27296, 60, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 3, 8267, 3, 15, 9, 3537, 3, 89, 4664, 147, 472, 5, 1838, 6, 213, 19, 34, 58, 29403, 71, 10, 3, 10354, 9, 7, 6, 11499, 6, 3519, 1496, 32, 17, 9, 6, 19343, 6, 6419, 1615, 19, 96, 1109, 1496, 32, 17, 9, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 3392, 2009, 19, 3, 9, 2021, 12662, 5, 156, 25, 174, 424, 1126, 68, 3627, 6, 125, 133, 25, 169, 58, 29403, 71, 10, 3, 63, 14547, 6, 4301, 19828, 6, 6442, 9568, 6, 10123, 49, 6, 20404, 3432, 1615, 19, 96, 5715, 19828, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 36, 9, 624, 19, 838, 4303, 7, 45, 3, 9, 5824, 2608, 6, 213, 19, 34, 1069, 58, 29403, 71, 10, 6179, 6029, 6, 30, 5310, 32, 6, 1335, 6, 19343, 6, 16, 3, 26165, 1615, 19, 96, 14710, 6029, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 600, 18051, 47, 8, 1530, 1466, 13, 8, 9823, 6, 34, 141, 66, 118, 1866, 21, 57, 3, 9, 5334, 12, 8, 125, 58, 29403, 71, 10, 690, 6, 538, 6, 851, 13, 2653, 6, 3, 11956, 6, 452, 286, 1615, 19, 96, 6726, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 7930, 164, 114, 46, 3490, 31, 7, 12517, 6, 78, 8, 3490, 164, 129, 474, 125, 58, 29403, 71, 10, 16, 1567, 13, 516, 6, 24025, 15264, 6, 281, 12, 496, 6, 1731, 161, 6, 765, 159, 32, 221, 1615, 19, 96, 77, 1567, 13, 516, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 4940, 19, 3140, 689, 250, 3, 88, 31, 7, 7718, 13, 5351, 12, 8, 3145, 31, 7, 5022, 6, 125, 429, 8, 3145, 428, 58, 29403, 71, 10, 19372, 6, 3, 5108, 16856, 6, 3169, 6, 43, 12, 456, 147, 6, 306, 874, 1615, 19, 96, 6225, 1273, 297, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58], [947, 31, 7, 3, 9, 822, 11, 3, 9, 360, 487, 4269, 10, 1593, 10, 71, 11738, 11, 13060, 33, 838, 124, 13, 12152, 6, 125, 19, 8, 952, 7252, 58, 29403, 71, 10, 3949, 6, 652, 161, 612, 6, 5281, 6, 8619, 95, 6, 772, 1615, 19, 96, 51, 10269, 545, 121, 46, 1525, 7901, 15, 26, 28, 936, 1017, 1254, 58]], 'inputs_pretokenized': ['Here\\'s a question and a few possible answers: \\n\\nQ: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\\nPossible A: park, coloring book, garden center, math problem, gravity\\n\\nWhy is \"math problem\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A John is a bum.  Much like the stereotype, he lives near this sort of transportation infrastructure. Where does he live?\\nPossible A: bus depot, beach, train station, bridge, bridge\\n\\nWhy is \"bridge\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A bad person places little value on being honest, acting without pretense or being what?\\nPossible A: excellent, upright, premium, competent, sincere\\n\\nWhy is \"sincere\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A bald eagle flies over St. Paul, where is it?\\nPossible A: texas, thermal, minnesota, canada, photograph\\n\\nWhy is \"minnesota\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A battleship is a powerful vessel.  If you need something similar but faster, what would you use?\\nPossible A: yatch, corvette, aircraft carrier, destroyer, patrol boat\\n\\nWhy is \"corvette\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A beaver is taking logs from a Pacific beach, where is it located?\\nPossible A: washington, ontario, books, canada, in russia\\n\\nWhy is \"washington\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A big fountain was the center piece of the renovation, it had all been paid for by a grant to the what?\\nPossible A: city, state, front of casino, rome, public place\\n\\nWhy is \"city\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A boss may like an employee\\'s ambition, so the employee may get put what?\\nPossible A: in charge of project, conquer opponent, go to school, begin work, webisode\\n\\nWhy is \"in charge of project\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A boy is leaving line because he\\'s tired of listening to the teacher\\'s orders, what might the teacher give?\\nPossible A: punishment, utter chaos, trouble, have to start over, high five\\n\\nWhy is \"punishment\" an answer aligned with human common sense? \\n', 'Here\\'s a question and a few possible answers: \\n\\nQ: A bride and groom are taking care of proposals, what is the likely ceremony?\\nPossible A: efficiency, getting work done, marriage, finishing up, results\\n\\nWhy is \"marriage\" an answer aligned with human common sense? \\n'], 'targets': [[765, 3357, 107, 19, 876, 12, 199, 25, 4602, 1], [8524, 51, 7, 33, 168, 801, 12, 240, 95, 6198, 365, 4716, 7, 5, 1], [48, 1448, 19, 167, 8318, 15990, 1], [3, 7, 17, 5, 102, 9, 83, 19, 3, 9, 5435, 16, 3519, 1496, 32, 17, 9, 1], [3, 99, 25, 174, 1634, 6, 4301, 19828, 19, 8, 1525, 5, 1], [6179, 6029, 19, 8, 163, 286, 16, 8, 570, 24, 65, 3, 5379, 3286, 9252, 1], [3, 9, 600, 18051, 47, 8, 1530, 1466, 13, 8, 9823, 6, 34, 141, 66, 118, 1866, 21, 57, 3, 9, 5334, 12, 8, 690, 1], [271, 17091, 598, 79, 56, 161, 614, 12, 36, 207, 5, 1], [19372, 21, 1028, 32, 4143, 23, 1433, 1], [8, 1657, 11738, 11, 13060, 19, 3323, 1968, 28, 3, 9, 5281, 7252, 1]], 'targets_pretokenized': ['\\nwebmath is designed to help you solve', '\\nbums are well known to take up residence under bridges.', '\\nthis word is most relavant', '\\nst.paul is a county in minnesota', '\\nif you need speed, corvette is the answer.', '\\nwashington is the only place in the list that has pacific beaches', '\\na big fountain was the center piece of the renovation, it had all been paid for by a grant to the city', '\\nbeing ambitious means they will work hard to be good.', '\\npunishment for disobedience', '\\nthe term bride and groom is mostly associated with a marriage ceremony']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15111/243029852.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(batch['targets']).long()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m targets \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(targets, (\u001b[39m0\u001b[39m, input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m targets\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, labels\u001b[39m=\u001b[39;49mtargets)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ppxscal/Projects/nlp/nlp_project/soft_prompting.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:682\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 682\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    683\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    684\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    685\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    686\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    687\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    688\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    689\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    690\u001b[0m )\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    692\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:597\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    595\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 597\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    600\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[1;32m    601\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:120\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39membeddings)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     input_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m seq_length \u001b[39m=\u001b[39m input_embeds\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a collate function to pad your sequences\n",
    "def collate_fn(batch):\n",
    "    inputs = [item['inputs'] for item in batch]\n",
    "    targets = [item['targets'] for item in batch]\n",
    "\n",
    "    # Tokenize the inputs and targets\n",
    "    inputs = [tokenizer.encode(input, return_tensors='pt')[0] for input in inputs]\n",
    "    targets = [tokenizer.encode(target, return_tensors='pt')[0] for target in targets]\n",
    "\n",
    "    # Pad the sequences\n",
    "    max_length_inputs = max([input.size(0) for input in inputs])\n",
    "    max_length_targets = max([target.size(0) for target in targets])\n",
    "    inputs = pad_sequence([torch.cat([input, input.new_zeros(max_length_inputs - input.size(0))]) for input in inputs], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets = pad_sequence([torch.cat([target, target.new_zeros(max_length_targets - target.size(0))]) for target in targets], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "\n",
    "# Use the DataLoader to batch your dataset\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset['train'], batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Define your soft prompt\n",
    "soft_prompt = \"Try to answer this: \"\n",
    "\n",
    "# Tokenize the soft prompt\n",
    "soft_prompt_tokens = tokenizer.encode(soft_prompt, return_tensors='pt')\n",
    "\n",
    "# Freeze the model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Initialize the soft prompt as a learnable parameter\n",
    "soft_prompt_param = torch.nn.Parameter(soft_prompt_tokens.float())\n",
    "soft_prompt_param.requires_grad = True\n",
    "\n",
    "# Define your optimizer to only update the soft prompt parameter\n",
    "optimizer = torch.optim.Adam([soft_prompt_param], lr=0.01)\n",
    "epochs = 1\n",
    "\n",
    "# Training loop\n",
    "print(dataset['train'][:10])\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        # Concatenate the soft prompt with the input\n",
    "        input_ids_batch = torch.Tensor(batch['inputs'])\n",
    "        soft_prompt_param_repeated = soft_prompt_param.repeat(input_ids_batch.size(0), 1)\n",
    "        input_ids = torch.cat([soft_prompt_param_repeated, input_ids_batch.long()], dim=-1)\n",
    "        input_ids = input_ids.long()\n",
    "\n",
    "        # Ensure the targets tensor has the same size as the input_ids tensor\n",
    "        targets = torch.tensor(batch['targets']).long()\n",
    "        targets = F.pad(targets, (0, input_ids.size(1) - targets.size(1)))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
