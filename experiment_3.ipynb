{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('bigscience/P3', 'cos_e_v1.11_aligned_with_common_sense')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "# Freeze the model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the soft prompt\n",
    "L = 8\n",
    "d = model.model.shared.embedding_dim\n",
    "soft_prompt = torch.nn.Parameter(torch.randn(L, d))\n",
    "optimizer = AdamW([soft_prompt])\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "soft_prompt.to(device)\n",
    "\n",
    "print('starting training')\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch['inputs_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "        labels = tokenizer(batch['targets_pretokenized'], return_tensors='pt', padding=True, truncation=True).input_ids \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Get the input embeddings\n",
    "        input_embeddings = model.model.shared(input_ids)\n",
    "\n",
    "        soft_prompt_batch = soft_prompt.unsqueeze(0).repeat(input_embeddings.size(0), 1, 1).to(device)\n",
    "        combined_embeddings = torch.cat([soft_prompt_batch, input_embeddings], dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the model\n",
    "        outputs = model(inputs_embeds=combined_embeddings, labels=labels)\n",
    "        # print(f'inpute embeddings shape: {input_embeddings.shape}')\n",
    "        # print(f'combined embeddings shape: {combined_embeddings.shape}')\n",
    "        # print(f'outputs shape: {outputs.logits.shape}')\n",
    "        # print(f'labels shape: {labels.shape}')\n",
    "        # print(f'labels: {labels}')\n",
    "        # print(f'labels pretokenized: {batch[\"targets_pretokenized\"]}')\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'\\r complete from this epoch {i}/{len(train_dataset)}', end='')\n",
    "        print(f'\\r loss: {loss.item()}', end='')\n",
    "\n",
    "\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(range(1, epochs + 1), losses)\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
