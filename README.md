https://www.overleaf.com/6948853616cnzbgdrrjwzb#da7fff

the goal for our project is to use soft prompting in a way that cuts costs while improving model prediction quality. We should try any new ideas we come up.

huggingface prompt-tuning?

The current research I've come accross suggests that the interpretability of soft prompts is a large area of interest. The problem is that while soft prompt vectors can be mapped to the closest tokens in the vocabulary, the resulting projected soft prompts are often arbitrary and nonsensical. Despite this, these nonsensical prompts can still perform well on tasks. 

Waywardness hypothesis?
